[
  {
    "objectID": "data-organisation.html",
    "href": "data-organisation.html",
    "title": "1) Organization",
    "section": "",
    "text": "Research data is valuable for researchers and forms the basis for their research. Therefore, it is advisable to structure the data well to save time and effort in the daily handling of research data. In this part of the workshop, we will look closer at organizational aspects of data management, mainly the folder structure, file and folder naming, and file formats.\nA clear and consistent folder structure and folder and file naming convention are important for making your data findable and interoperable. You should think about it beforehand in order to avoid inconsistencies or the need to rename large amounts of data.\nYour structure and your naming conventions should be intuitive. However, we recommend to explicitly describe them (typically in a README file) because they may not be that intuitive for others or your future self (“why did I do it like this?”).\nIn the following sections, you’ll find some input on the organizational aspects you should consider. Note that not all of them may apply to each dataset. Besides the tasks, we’ll provide some general hints and rules. Some rules only apply to some use cases, and sometimes, there are good arguments for not sticking to every rule. However, in such cases you should know (and potentially document) why you decide differently.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#general-notes",
    "href": "data-organisation.html#general-notes",
    "title": "1) Organization",
    "section": "",
    "text": "Research data is valuable for researchers and forms the basis for their research. Therefore, it is advisable to structure the data well to save time and effort in the daily handling of research data. In this part of the workshop, we will look closer at organizational aspects of data management, mainly the folder structure, file and folder naming, and file formats.\nA clear and consistent folder structure and folder and file naming convention are important for making your data findable and interoperable. You should think about it beforehand in order to avoid inconsistencies or the need to rename large amounts of data.\nYour structure and your naming conventions should be intuitive. However, we recommend to explicitly describe them (typically in a README file) because they may not be that intuitive for others or your future self (“why did I do it like this?”).\nIn the following sections, you’ll find some input on the organizational aspects you should consider. Note that not all of them may apply to each dataset. Besides the tasks, we’ll provide some general hints and rules. Some rules only apply to some use cases, and sometimes, there are good arguments for not sticking to every rule. However, in such cases you should know (and potentially document) why you decide differently.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#folder-structure",
    "href": "data-organisation.html#folder-structure",
    "title": "1) Organization",
    "section": "Folder structure",
    "text": "Folder structure\nAt the start of your research project, you have to decide how to arrange your files and folders. This decision depends on the structure of your data and documentation. Organizational choices may involve trade-offs, such as the number of files per folder versus folder depth, intuitive names versus strict naming conventions, and structuring by processing level, access permissions, file size, or other criteria.\n\n\n\n\n\n\nTask 1.1: (~ 5 minutes)\n\n\n\nLook at the folder structure (but not at the details of the folder or file names yet, which will be the next task).\n\nIs the folder structure intuitive and logical (what is done, how, and why)?\nIs it explicitly described? Where can you find this information (metadata of repository or in a README file)?\nHow many files are stored per folder, and how deeply are they nested?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\nIn case there are no folders, you may discuss whether it would make sense to add folders.\n\n\n\n\n\n\n\n\nAvoid too long file paths\n\n\n\nDepending on the operating system, the total path length has an upper limit, e.g. 255 characters. Exceeding this limit will cause errors. Also note that the path of the copy may be even longer than your original path if you synchronize or backup your data, which can cause your sync or backup job to fail. Therefore, try to keep your full path clearly below such upper limits.\n\nBad example: X:/Projects/Microscopy_Project/Microscopy_Projects_2024/October_2024/RawData_October2024/Microscopy_RawData_Image003.tif\nBetter: X:/Projects/Microscopy/2024-10/RawData/Image003.tif\n\n\n\n\n\n\n\n\n\nFurther hints on folder structure\n\n\n\n\nAvoid deeply nested folder structures: SubSubSubSubSubFolders can be pretty inconvenient.\nAvoid too many files or subfolders within one folder:\nIt can be quite inconvenient to look through dozens of heterogeneous file names. In case of clearly structured file names (e.g. numbered files like Image003.tif or Plot01_Part03.tab), a larger number of elements per folder can also be fine. However, for huge amounts of files (several thousand), the performance of the file explorer may decrease.\nIn case different project members should have different access restrictions to files, this could also be considered in your folder structure.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\nExample for structuring a dataset: organized by file type1\n+ DatasetA\n  + Data\n    + Processed\n    + Raw\n  + Results\n    + Figure1.tif\n    + Figure2.tif\nExample for structuring a dataset: organized by analysis2\n+ DatasetB\n  + Figure1\n    + RawData\n    + Results\n      + Figure1.tif\n  + Figure2\n    + RawData\n    + Results\n      + Figure2.tif\nExample for a project folder structure3:\n+ Project_Folder\n  + 1_Project_Management\n    + Finance\n    + Proposals\n    + Reports\n  + 2_Ethics_and_Governance\n    + Consent_Forms\n    + Ethical_Approvals\n  + 3_Dissemination\n    + Presentations\n    + Publications\n    + Publicity\n  + Experiment_01\n    + Data\n    + Data_Analysis\n    + Inputs\n    + Outputs\nExample for a project folder structure4:\n\\\\file.mpic.de\\projects\\ExampleProject\\\n  + GeneralOverview  # General documentation of the project\n  + Meetings         # Meeting notes, presentations\n  + INST             # Instruments\n    + Instrument1    # One folder per instrument\n      + Doc          # Documentation for this instrument\n      + L_0          # Raw data\n      + L_2          # Processed/analyzed data on original resolution\n        + Product1   # One folder per data product\n          + Code     # Code used for creating the data of this product\n          + Data     # Data files of this product\n          + Doc      # Documentation for this data product\n      + L_3          # Gridded data products\n        + Product1   # One folder per data product (e.g. hourly averages)\n          + Code     # Code used for creating the data of this product\n          + Data     # Data files of this product\n          + Doc      # Documentation for this data product\n      + Labbook      # Labbook (photos of paper logbook or exports from ELN)\n\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\n\nIs the folder structure intuitive and logical (what is done, how, and why)?\nIs it explicitly described? Where can you find this information (metadata of repository or in a README file)?\nHow many files are stored per folder, and how deeply are they nested?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\nIn case there are no folders, you may discuss whether it would make sense to add folders.\n\nThe dataset has 42 files, but no folder structure. Folders are not needed here, because all files (except for the README file) are of same type, just for different months. However, one could make one subfolder per year.\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\n\nIs the folder structure intuitive and logical (what is done, how, and why)?\nIs it explicitly described? Where can you find this information (metadata of repository or in a README file)?\nHow many files are stored per folder, and how deeply are they nested?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\nIn case there are no folders, you may discuss whether it would make sense to add folders.\n\nThe dataset contains 6 files, whithout folder structure. However, 2 of them are of type ‘tar.gz’, which contain compressed ASCII files. The content is described in the README file. Also the tar.gz files do not contain many files, thus no further folder structure is needed.\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\n\nIs the folder structure intuitive and logical (what is done, how, and why)?\nIs it explicitly described? Where can you find this information (metadata of repository or in a README file)?\nHow many files are stored per folder, and how deeply are they nested?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\nFollowing notes relate to the content of OSF Storage.\n\nYes, files are grouped into data, code (scripts) etc.\nThe content is described in the README file, but not completely.\nThere are up to 8 files per folder, one folder level.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\n\nIs the folder structure intuitive and logical (what is done, how, and why)?\nIs it explicitly described? Where can you find this information (metadata of repository or in a README file)?\nHow many files are stored per folder, and how deeply are they nested?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\nOn dataset-level, there is only one zip file, no folders. However, within the zip file, there is a folder structure:\n\nYes, intuitive structure: separation between data tables and scripts, …\nExplicitly described in README file.\nAround 3 to 4 folder levels. Folder Data/Group has 40 subfolders with 9 files each.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#file-and-folder-names",
    "href": "data-organisation.html#file-and-folder-names",
    "title": "1) Organization",
    "section": "File and folder names",
    "text": "File and folder names\nIn the next section, we will explore best practices for file and folder naming to create a clear and organized data structure. File or folder names have the following primary purposes:\n\nAlways: Uniquely identify the file or folder (within a folder),\nOften: Give information about its content, e.g. README.txt, MeetingProtocol.docx, Temperature_RawData.tab,\nSometimes: Enable logical order when sorting alphabetically, e.g. 1_RawData, 2_PreProcessed, 3_Processed, 4_Combined.\n\nGenerally, the same rules apply to the naming of folders and files. They shall allow to choose the desired file amongst all the other files of the folder. Therefore, the names should be concise and intuitive (if applicable). For instance, a file named XYZ123 might not be immediately clear, so it’s important to explain its purpose somewhere, typically in a README file. Well-structured folders have clear naming conventions, which are explicitly described.\n\n\n\n\n\n\nTask 1.2: (~ 10 minutes)\n\n\n\n\nWhat naming convention is used in this dataset? Is it intuitive and logical? Is it explicitly described?\nAre the names meaningful? Are there misleading names?\nIn case of multiple files: Do they appear in a logical order when sorted alphabetically?\nAre there problematic characters like spaces, non-ASCII characters, etc.?\nWhat about the length of the names?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\n\n\n\n\n\n\n\nDo not use bad characters.\n\n\n\nDepending on the operating system and application, some characters are forbidden or may lead to problems and, thus, should be avoided.\n\nVery bad: Any non-ASCII character, e.g., öäüßµαδ°±•€→☺É\nBad: Any whitespace character, e.g. File 1.txt. They can cause problems, e.g., in some batch tasks, in particular, if one forgets to surround the name with quotes. Furthermore, double or multiple spaces and spaces at the beginning of the name are not clearly visible.\nForbidden in Windows: \\/:*?\"&lt;&gt;|\nAlso not recommended: ,;()[]{} etc.\n\nTo summarize: You should only use Latin letters A-Z, a-z, digits 0-9, underscore, hyphen and dot, i.e. following characters: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz123456789_-.\nFurthermore, the dot should only be used in file names, and there only once before the file extension, e.g. “Notes.txt”. Some programs use a dot or underscore as the first character for special file types, e.g. _quarto.yml or .git and thus should be avoided for regular data files.\n\n\n\n\n\n\n\n\nNo ‘hello.txt’ and ‘Hello.txt’ in same folder.\n\n\n\nEnsure that subfolders and files have unique names within a folder, even in case-insensitive ways. For example, do not put two files named hello.txt and Hello.txt in the same folder.\nThis note is particularly relevant for Linux users, where putting both files in the same folder is possible. However, in Windows, that is not allowed. Thus, sharing such a folder between users of different operating systems would cause problems.\n\n\n\n\n\n\n\n\nExcursion: Ordering and timestamps\n\n\n\nA naming convention can enable a logical order of the file or folder names when sorting them alphabetically. Here, we provide some tips:\n\nWhen names include numbers, leading zeros are often helpful:\n\nOrdering with “0”:\nScan01.csv, Scan02.csv, Scan03.csv, Scan04.csv, Scan05.csv, Scan06.csv,\nScan07.csv, Scan08.csv, Scan09.csv, Scan10.csv, Scan11.csv, Scan12.csv\nOrdering without:\nScan1.csv, Scan10.csv, Scan11.csv, Scan12.csv, Scan2.csv, Scan3.csv,\nScan4.csv, Scan5.csv, Scan6.csv, Scan7.csv, Scan8.csv, Scan9.csv\n\nTimestamps should always be given with a leading zero and ‘from big to small’, i.e. year, month, day of month, hour, minute, second. This recommendation complies with the international format ISO 8601 (e.g. “2024-07-31”, “2024-07-31T2313”).\n\nVery bad: 13Jan2024, 21April2021, 3Dec2025\nAlso bad: 03122025, 13012024, 21042021\nGood: 2021-04-21, 2024-01-13, 2025-12-03\nAlso ok: 20210421, 20240113, 20251203\nIncluding time of day: 20210421T0345, 20240113T1730, 20251203T1900 for 03:45, 17:30, 19:00\n\n\n\n\n\n\n\n\n\n\nFurther good practice for file naming\n\n\n\n\nInclude relevant information in the file name. However, don’t misuse a file name as a way to store all your metadata.\nAvoid overly long names (a maximum of 32 characters is suggested). Mind also the previous note about the full path length.\nAvoid moving or renaming folders or files. This is especially relevant when you or others have referred to the file by using its file name or path.\nGenerate a README file explaining file nomenclature (including the meaning of acronyms or abbreviations), file organization and versioning. Store this file on top of the folder structure for easy accessibility.\n\nThere are different possibilities to indicate logical units in a name without using a whitespace:\n\nKebab-case: The-quick-brown-fox-jumps-over-the-lazy-dog.txt\nCamelCase: TheQuickBrownFoxJumpsOverTheLazyDog.txt\nSnake_case: The_quick_brown_fox_jumps_over_the_lazy_dog.txt\n\nCompromises often have to be made, such as including relevant information versus avoiding long names. Note that folder names with a precise and narrow meaning may become outdated when further content is filled in over time. Because of that, persistent identifiers (PID) typically avoid to include semantic information, e.g. doi:10.17617/3.1STIJV.\n\n\n\n\n\n\n\n\nExcursion: Versioning\n\n\n\nDocuments may evolve over time. File versioning allows for reverting to earlier versions if needed and shall allow for keeping track of changes, including documentation on the underlying rationale and people involved.\nVersion control can be done either manually by using naming conventions or by using a version control system like Git. The following hints apply to manual version control, meaning that you store both the current and previous versions in your file system.\n\nVersions should be numbered consecutively, e.g. Handbook_v3.pdf. Major changes (v1, v2, v3, …) can be distinguished from minor ones (v1-1, v1-2, v1-3 or 1a, 1b, 1c). You may use leading zeros if you expect more than nine versions.\nAlternatively, a date or timestamp could indicate the version, e.g. Handbook_v20240725.pdf.\nYou may use qualifiers such as “raw” or “processed” for data or “draft” or “internal” for documents. However, note that terms such as “final”, “final2”, “final-revised”, “final-changed_again”, and “final_ready” can be confusing. In other words: Avoid the word “final” in file names.\nDocument your versioning convention, e.g. what you mean with major or minor changes.\nDocument the essential changes you have made between the versions.\n\nFor further reading: GitHub recommends version names like ‘1.3.2’ for the releases of software products, details see Semantic Versioning 2.0.0.\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\n\nWhat is the convention of the naming used in this dataset? Is it intuitive and logical? Is it explicitly described?\nAre the names meaningful? Are there misleading names?\nIn case of multiple files: Do they appear in a logical order when sorted alphabetically?\nAre there problematic characters like spaces, non-ASCII characters, etc.?\nWhat about the length of the names?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\n\nFiles consist of prefix amb_hourly_qc_wc4.4_cal6.0_, followed by year and month (e.g. 2014_08), followed by _core-params.csv. Prefix might be intuitive for researchers of that field, but it is not explicitly described.\nProbably the prefix has some meaning, but as it is not explicitly stated in the README, we can only speculate.\nYes, files are sorted according to the month of measurement.\nYes: File name AMB hourly, readme.rtf contains spaces and a comma. The other file names contain several dots (should only be one dot, namely before the file extension csv).\nLength is not problematic, but longer than needed.\nReplace AMB hourly, readme.rtf by README.rtf. The other file names can be shortened, e.g. HourlyCoreParams_2014_08.csv. And if the file name prefix amb_hourly_qc_wc4.4_cal6.0_ contains relevant information, this should be explicitly given in the metadata or README file.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\n\nWhat is the convention of the naming used in this dataset? Is it intuitive and logical? Is it explicitly described?\nAre the names meaningful? Are there misleading names?\nIn case of multiple files: Do they appear in a logical order when sorted alphabetically?\nAre there problematic characters like spaces, non-ASCII characters, etc.?\nWhat about the length of the names?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\n\nThe dataset contains only 6 files, thus there is not really a convention available, and also not needed. The non-intuitive parts wos and fo are explained in the README file (namely “Web of Science data” and “Faculty Opinions data”). The files inside the tar.gz-files seem to follow some convention, and their content is explicitly mentioned in the README file.\nProbably yes. Anyhow, their content is mentioned in the README file.\nOnly few files, does order not important.\nNo problematic characters found.\nLength of the names: OK\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\n\nWhat is the convention of the naming used in this dataset? Is it intuitive and logical? Is it explicitly described?\nAre the names meaningful? Are there misleading names?\nIn case of multiple files: Do they appear in a logical order when sorted alphabetically?\nAre there problematic characters like spaces, non-ASCII characters, etc.?\nWhat about the length of the names?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\nFollowing notes relate to the content of OSF Storage.\n\nNot so clear, but not many files, thus no clear conventions needed. However, the files in folder result are lacking an explanation in the README file, and their names are not very intuitive.\nMeaning not clear for all files.\nOnly few files, thus order not important.\nNo problematic characters found in OSF Storage.\nLength of the names: OK\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\n\nWhat is the convention of the naming used in this dataset? Is it intuitive and logical? Is it explicitly described?\nAre the names meaningful? Are there misleading names?\nIn case of multiple files: Do they appear in a logical order when sorted alphabetically?\nAre there problematic characters like spaces, non-ASCII characters, etc.?\nWhat about the length of the names?\nDiscuss: What would you leave as it is, what would you change, or what are the alternatives?\n\n\n\nThe subfolders of Data/Group and Data/Solo have names like 01_09_2022__10_13_33, which seem to refer to a date and maye time of day.\nYes, names are meaningful and intuitive.\nSubfolders are not in a chronological order, because the date is given in a disadvantageous format (e.g. 01_09_2022) - better would be 2022_09_01 or 2022-09-01.\nYes: Folder name Stan model code contains spaces.\nLength of the names: OK\nIf folder name 01_09_2022__10_13_33 stands for timestamp 2022-09-01T10:13:33, then it could be renamed to 20220901T101333 or 2022-09-01_101333.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#file-formats",
    "href": "data-organisation.html#file-formats",
    "title": "1) Organization",
    "section": "File formats",
    "text": "File formats\nA file format has to be chosen when storing information in a file. It builds the backbone of your data and is usually specified by the file extension (e.g. .txt). To keep your data interoperable, the format needs a clear structure. This makes your data easy to read with many software products (e.g., out-of-the-box solutions or by writing a small script). Clear documentation of the file format shall be publicly available. Considering all these aspects, the chance is high that the file can be read in future, making it suitable for long-term preservation - which is one of our main goals when managing data. Therefore, open file formats are recommended, while proprietary formats should be avoided.\nIdeally, when choosing a suitable format, you’ll consider the following properties:\n\nReadable by humans with a simple editor\nReadable with many programs\nEasy to understand, low complexity\nSmall (storage space)\nQuick to read (performance)\n\nHowever, usually compromises have to be made. For example, binary files are generally more performant than csv files and thus more suitable during the active research process. At the same time, csv is a well-established format for long-term preservation and is easier for humans to read.\n\n\n\n\n\n\nTask 1.3: (~ 10 minutes)\n\n\n\n\nAre the files stored in an open or a proprietary format?\nIs the file format used “future-proof”, e.g., suitable for long-term archiving?\nHow easy is it to open the file (regarding available programs and file size)?\nHow complex are the files? What is their internal structure?\nWhat about performance and file size?\nHow easy is it to understand the file as humans?\nAre they machine-readable and standardized? How easy is it to write a script to read the files?\nWhich alternative formats exist?\n\n\n\n\n\n\n\n\n\nAvoid proprietary formats.\n\n\n\nOften, proprietary formats have intentionally no proper documentation as the company behind the system wants to keep their business information behind closed doors. The companies sometimes even use technical protection mechanisms, making the file format readable only by commercial software. This reduces the interoperability and reusability of the files and, in the worst case, makes them unreadable in the long term. (Imagine the company that provided the software and file format no longer exists.) Furthermore, the files might contain hidden (potentially sensitive) information. Thus, such formats should be avoided.\n\n\n\n\n\n\n\n\nExamples of recommended formats\n\n\n\nIn the following list, you’ll find some formats which are widely used, well-documented and readable with several programs.\n\nFor documentation:\n\nPlain text (.txt)\nHTML, XHTML, Markdown\nPDF (PDF/A-1)\nmaybe: Rich Text Format (.rtf), Open Document Text (.odt), docx, …\n\nTabular data:\n\nComma-separated values (.csv)\nTab-delimited (.tab)\nmaybe: Open Document Spreadsheet (.ods), xlsx, …\n\nNested data:\n\nJSON\nXML\n\nFurther formats:\n\nNetCDF, HDF5, …\npng, jpg, …\n\n\nNotes:\n\nPDF: PDF has been developed by Adobe Inc. and thus originally had been a proprietary format, and several versions exist. Nevertheless, the format is widely used today. For archival purposes, a PDF/A version is the best choice. PDF is best suited for fixed documentation. However, editing PDF files or extracting data from them takes a lot of work.\nSpreadsheet files: Spreadsheets may look nice, particularly when formatted in a colourful way. But for the machine-readability, this can cause problems. In particular, we do not recommend that you present relevant information just by formatting content differently. You can take this as a rule of thumb: Spreadsheet files like .xlsx or .ods are not well machine-readable.\n\n\n\n\n\n\n\n\n\nExcursion: Premium format ASCII\n\n\n\n\n\nA gold standard for storing digital information is an ASCII file. In an ASCII file, each byte represents one visible character (except for the white spaces and control characters like tab stop and linebreaks).\nTherefore, ASCII files can be read or opened by any text editor or data-processing software, even with programs like Excel, Word, Wordpad or web browsers (only possibly limited regarding the file size).\nCharacters beyond ASCII:\nAn ASCII file can only contain the following visible characters: !\"#$%&'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ Otherwise, it is not an ASCII file.\nFor some years, the Unicode-based file format “UTF-8” has been available, which can represent many characters beyond the ASCII characters, like “ü”, “€”, and even some smilies ☺. Nowadays, UTF-8 is supported by many editors and browsers. The good thing about UTF-8 is that as long as a UTF-8 file contains only ASCII characters, the UTF-8 file is automatically an ASCII file. In other words, an ASCII file is a super-interoperable UTF-8 file.\n\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\n\nAre the files stored in an open or a proprietary format?\nIs the file format used “future-proof”, e.g., suitable for long-term archiving?\nHow easy is it to open the file (regarding available programs and file size)?\nHow complex are the files? What is their internal structure?\nWhat about performance and file size?\nHow easy is it to understand the file as humans?\nAre they machine-readable and standardized? How easy is it to write a script to read the files?\nWhich alternative formats exist?\n\n\n\nFiles are ASCII files, thus open.\nYes, ASCII is suitable for long-term archiving.\nEasy to open, e.g. with text editor.\nFiles have tabular shape.\nOK, file sizes are below 1 MB.\nShape: easy to understand, meaning of the columns given in README file.\nMost data analysis programs have import functions for csv. The quotes in the first column might be cumbersome for some import routines.\nTab-separated files, spreadsheet files, etc.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\n\nAre the files stored in an open or a proprietary format?\nIs the file format used “future-proof”, e.g., suitable for long-term archiving?\nHow easy is it to open the file (regarding available programs and file size)?\nHow complex are the files? What is their internal structure?\nWhat about performance and file size?\nHow easy is it to understand the file as humans?\nAre they machine-readable and standardized? How easy is it to write a script to read the files?\nWhich alternative formats exist?\n\n\n\nThe small files are ASCII or UTF-8 files, thus open. The tar.gz files are compressed TAR-files, thus also in an open format.\nYes, ASCII is definitively suitable for long-term archiving. Also tar.gz files are widely used and can thus be considered suitable for long-term archiving.\nThe tar.gz files need specific software for extraction, which is freely available, but maybe not installed everywhere, and not all people are familiar with. Thus it is commandable that the extaction is described in the README file. However, the file size of several GB can be problematic for users having a slow internet connection. And unpacked, the largest file is more than 26 GB, more than the RAM size of many computers.\nThe data files (inside the tar.gz) are not complex, just tables.\nDue to compression, the file size is reduced for storage and download. However, the tables contain many digits, probably more than needed. Reducing them would decrease file size. Binary files instead of ASCII files would need less time for loading.\nShape: easy to understand, meaning of column see README file.\nMost data analysis programs have import functions for csv.\nBinary files like HDF, which could enhance performance.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\n\nAre the files stored in an open or a proprietary format?\nIs the file format used “future-proof”, e.g., suitable for long-term archiving?\nHow easy is it to open the file (regarding available programs and file size)?\nHow complex are the files? What is their internal structure?\nWhat about performance and file size?\nHow easy is it to understand the file as humans?\nAre they machine-readable and standardized? How easy is it to write a script to read the files?\nWhich alternative formats exist?\n\n\nFollowing notes relate to the content of “OSF Storage”.\n\nMost files are in an open format: ASCII tables, JSON files, R scripts. But what are “nii.gz” files in folder “results” - maybe zipped NIfTI files?\nYes for ASCII tables and JSON files; maybe yes for nii.gz files.\nASCII tables and JSON files: easy to open with every text editor, special software or libraries needed for nii.gz.\nFiles in folder data are tables (csv) or Codebooks (in JSON format) describing those.\nOK, because the files are not very large.\nASCII tables and JSON files are easy to understand by humans; nii.gz needs suitable software.\nMost data analysis programs have import functions for csv, also JSON import functions are available for several programs.\nFor csv-tables: Tab-separated files, spreadsheet files, etc; for JSON: XML\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\n\nAre the files stored in an open or a proprietary format?\nIs the file format used “future-proof”, e.g., suitable for long-term archiving?\nHow easy is it to open the file (regarding available programs and file size)?\nHow complex are the files? What is their internal structure?\nWhat about performance and file size?\nHow easy is it to understand the file as humans?\nAre they machine-readable and standardized? How easy is it to write a script to read the files?\nWhich alternative formats exist?\n\n\n\nFiles are stored as ASCII tables or plain text files, which are open formats.\nYes, suitable for long-term archiving.\nEasy, readable with text editor.\nData files are ASCII tables.\nDue to compression, the file size is reduced for storage and download. Binary files instead of ASCII files would need less time for loading.\nThe format is easy to understand by humans, but the columns are not explicitly described.\nMost data analysis programs have import functions for semicolon-separated tables.\nBinary files like HDF could be used (cf note above related to performance).\n\n\n\n\n\nSpecial file types: tabular text file (optional)\nPlease note that the task in this section is optional. You can go through this section if you still have some time left during the workshop or read it afterwards.\nTabular text files store data in a structured format, where each row represents a record and each column represents a field, with data separated by a designated column separator. Even after deciding to store tabular data in text files (e.g. files which can be opened in any editor), there are various ways and conventions to choose from:\n\nColumn separator: typically tab or comma, sometimes space or semicolon\nNumeric values: handling of missing values (e.g. “NA”, ““, etc.)\nRepresentation of timestamps, e.g. “2024-08-01T08:59”\nHeader lines with meta information?\nEncoding: Recommended is ASCII or UTF-8\n\n\n\n\n\n\n\nTask 1.4: (~ 5 minutes)\n\n\n\n\nHow is the file encoded (e.g. ASCII, UTF-8)?\nNumbers: What about their precision (enough or too much)?\nSpecial numbers: Do special numbers like “NA”, ““,”N/A”, “999”, “0” occur? Is their meaning documented?\nTime: Which format is used for the date and time of day? Which time zone is used?\nTables: What do you generally notice (regarding the choice of separator, whitespaces, missing columns)?\nIs the content of the table self-explaining? Is there a column description? What units were used? Is there detailed information elsewhere?\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFirst, you will find an example of a very bad file, followed by an improved version.\nFile Measured last month.txt:\ndate, time,sensor,sensor\n03/07/24 12.00 AM,17.3\n03/07/24 1.00 AM,16.9\n03/07/24 2.00 AM,16.7\n03/07/24 3.00 AM,16.4\n03/07/24 4.00 AM,16.2\n03/07/24 5.00 AM,15.9\n03/07/24 6.00 AM\n03/07/24 7.00 AM\n03/07/24 8.00 AM\n03/07/24 9.00 AM,16.5\n03/07/24 10.00 AM,17.0,7.2\n03/07/24 11.00 AM,17.6,4.6\n03/07/24 12.00 PM,18.0\n03/07/24 1.00 PM,18.5\nWe gathered some comments on that file:\n\nFirst, we notice that the file name is bad. It contains spaces, and “last month” is no meaningful name (which month is considered as the actual one?).\nThat file is not a proper csv file because it does not have a proper tabular shape:\n\nThe header line indicates that we have 4 columns. When looking at the data, one can assume that there is one comma too much as the date and time of day are stored in one column.\nFurther, we have at most one comma leading into two columns in the data rows. This does not match the header. Therefore, we can assume that some values are missing.\n\nThe header line contains twice the word “sensor”. Thus, the column names are not unique.\nThe time column is horrible:\n\nThe date is given in an ambiguous format: you do not know if it is 03 July 2024 or 07 March 2024 or 24 July 2003, or 1924 or in the year 24 AD? You should use the international format ISO 8601, here “2024-03-07” for 7 March 2024.\nTime is given according to 12h clock with “AM” and “PM”. Hint: Never use “AM” or “PM” in a scientific context. Always use 24h clock!\nThe hour is noted without a leading zero.\nBetween hour and minute, a dot is used. It would be better to use a colon, e.g. “00:00”.\n\nImportant information is missing (but might be given in a separate README file or Codebook), e.g.:\n\nTime zone?\nWhat is the file about?\nWhy are values missing?\nUnit?\n\n\nAnd here is a better version: File Temp_Rain_202407.csv:\n# Averaged temperature and precipitation of Ex_Emplum station\n#\n# File created on 2024-04-22 by Schlaubi Schlumpf.\n# This file contains temperature and precipitation measured at the fictitious weather station 'Ex_Emplum' at 55.432 degrees North, 55.678 degrees East.\n# Raw data have been averaged over 1 hour.\n# NA indicates missing values due to measurement interruption or instrument malfunction.\n# Column description:\n#   - Time: Start time of the 1-hour interval, given as UTC, in ISO 8601 format 'YYYY-MM-DDThh:mm'.\n#   - Temp: Temperature at 2 m above ground level, averaged over the 1-hour interval, in degrees Celsius. The error of the given value is expected to be below 0.3 degrees Celsius.\n#   - Rain: Precipitation height accumulated within the 1-hour interval, in mm. The error of the given value is expected to be below 0.5 mm.\n#\nTime,Temp,Rain\n2024-03-07T00:00,17.3,0\n2024-03-07T01:00,16.9,0\n2024-03-07T02:00,16.7,0\n2024-03-07T03:00,16.4,0\n2024-03-07T04:00,16.2,0\n2024-03-07T05:00,15.9,0\n2024-03-07T06:00,NA,NA\n2024-03-07T07:00,NA,NA\n2024-03-07T08:00,NA,NA\n2024-03-07T09:00,16.5,0\n2024-03-07T10:00,17.0,7.2\n2024-03-07T11:00,17.6,4.6\n2024-03-07T12:00,18.0,0\n2024-03-07T13:00,18.5,0\n\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\n\nHow is the file encoded (e.g. ASCII, UTF-8)?\nNumbers: What about their precision (enough or too much)?\nSpecial numbers: Do special numbers like “NA”, ““,”N/A”, “999”, “0” occur? Is their meaning documented?\nTime: Which format is used for the date and time of day? Which time zone is used?\nTables: What do you generally notice (regarding the choice of separator, whitespaces, missing columns)?\nIs the content of the table self-explaining? Is there a column description? What units were used? Is there detailed information elsewhere?\n\n\n\nASCII\nHas many digits, e.g. 986.223944276841.\nNo information about missing values found in README file. But file amb_hourly_qc_wc4.4_cal6.0_2017_03_core-params.csv contains NA.\nTime is ISO 8601 conform, except that a space is given between date and time of day, e.g. 2017-03-23 09:30:00. In readme file mentioned: “All times given in GMT”.\nComma as column separator, whitespace only between date and time of day, no missing columns found.\nNot self-explaining but mentioned in README file, also the units.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\n\nHow is the file encoded (e.g. ASCII, UTF-8)?\nNumbers: What about their precision (enough or too much)?\nSpecial numbers: Do special numbers like “NA”, ““,”N/A”, “999”, “0” occur? Is their meaning documented?\nTime: Which format is used for the date and time of day? Which time zone is used?\nTables: What do you generally notice (regarding the choice of separator, whitespaces, missing columns)?\nIs the content of the table self-explaining? Is there a column description? What units were used? Is there detailed information elsewhere?\n\n\n\nUTF-8, except for the tar.gz. The files inside those tar.gz are even ASCII files.\nProbably more digits than needed, e.g. -13.333333333333336. Considering the file size, shortening them could be worthwile.\nNo information about missing values found in README file. But NA found in several files.\nTime: There seems to be no time column.\nTables: Comma as column separator, no missing columns or whitespaces found.\nNot self-explaining but columns mentioned in README file.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\n\nHow is the file encoded (e.g. ASCII, UTF-8)?\nNumbers: What about their precision (enough or too much)?\nSpecial numbers: Do special numbers like “NA”, ““,”N/A”, “999”, “0” occur? Is their meaning documented?\nTime: Which format is used for the date and time of day? Which time zone is used?\nTables: What do you generally notice (regarding the choice of separator, whitespaces, missing columns)?\nIs the content of the table self-explaining? Is there a column description? What units were used? Is there detailed information elsewhere?\n\n\nFollowing notes relate to the content of OSF Storage.\n\nEncoding: ASCII files (except for the nii.gz files)\nNumbers: e.g. 0.878519 - looks reasonable\nNo information about missing values found in README file. But NA found in several files.\nTime: There seems to be no time column.\nTables: Comma as column separator, no missing columns or whitespaces found.\nContent of the table is explained in JSON file (Codebook).\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\n\nHow is the file encoded (e.g. ASCII, UTF-8)?\nNumbers: What about their precision (enough or too much)?\nSpecial numbers: Do special numbers like “NA”, ““,”N/A”, “999”, “0” occur? Is their meaning documented?\nTime: Which format is used for the date and time of day? Which time zone is used?\nTables: What do you generally notice (regarding the choice of separator, whitespaces, missing columns)?\nIs the content of the table self-explaining? Is there a column description? What units were used? Is there detailed information elsewhere?\n\n\n\nData tables are ASCII files.\nNumbers: e.g. 73.12958 - looks reasonable\nSpecial numbers or contents: Some Columns contain parenthesis - what is their meaning?\nTime: Time column with seconds(?) since start time?\nTables: Semicolon as column separator, also semicolon after last column.\nThe README file says “Variable names should be quite descriptive, but please get in touch in case anything is unclear”, but not all columns are so clear to understand.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#references",
    "href": "data-organisation.html#references",
    "title": "1) Organization",
    "section": "References",
    "text": "References\nExamples and notes have been adapted from: Onboarding into Research Data Management, Franke et al. 2024, https://hdl.handle.net/21.11116/0000-000E-194D-1, file “FDM-Onboarding-2024_CPT-Slides.pdf” pages 44-51, 56-59.",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "data-organisation.html#footnotes",
    "href": "data-organisation.html#footnotes",
    "title": "1) Organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nadapted from https://datadryad.org/stash/best_practices#organize↩︎\nadapted from https://datadryad.org/stash/best_practices#organize↩︎\nadapted from Suse Prejawa (2021, https://hdl.handle.net/21.11116/0000-0008-662A-7)↩︎\nadapted from a template used at the Max Planck Institute for Chemistry for measurement projects/campaigns (e.g. with a research aircraft)↩︎",
    "crumbs": [
      "About",
      "1) Organization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "We welcome you to the FAIR research data management workshop. This interactive course is centered around the FAIR principles and their practical application in managing your research data efficiently. FAIR is an acronym and encompasses:\nThe FAIR data principles state that it should be easy to find research data, they should contain information about how to gain access to them, they should be compatible with other data, and possible to reuse.1 For instance, ensuring your data is well-organized and documented can make it easier for humans and machines to find and understand it and enable better reuse of the data. If the concept of the FAIR principles is new to you, take a minute to check this comprehensive overview of the GO FAIR initiative before starting this workshop.",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-is-this-workshop-for",
    "href": "index.html#who-is-this-workshop-for",
    "title": "Welcome",
    "section": "Who is this workshop for?",
    "text": "Who is this workshop for?\nThis workshop was designed for early career researchers who are either about to start or want to manage their empirical research data more effectively. Although it was developed with this target group in mind, it can most probably benefit many other researchers who are also consolidating their data management skills.",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-the-purpose-of-this-workshop",
    "href": "index.html#what-is-the-purpose-of-this-workshop",
    "title": "Welcome",
    "section": "What is the purpose of this workshop?",
    "text": "What is the purpose of this workshop?\nAppreciation of adequate data management usually occurs when looking into someone else’s data folder:\n\n\n\nPiled Higher and Deeper by Jorge Cham\n\n\nHowever, structures like this evolve over time and especially in the beginning of a project it is hard to imagine ending up with such a chaotic folder. Yet, more advanced participants can potentially think of an example where a folder tended to look similar, only that you are mastering your structures better than others.\nThis workshop will artificially advance your experience of navigating other researcher’s data, thereby hoping to point out good and bad practices that shall soon inspire your own data management. It also highlights the importance of planing the management of your data very early in the course of the project to benefit most from it: Having well-organized and documented data will not only increase their value and accessibility for the wider research community but also for your future self, when going back into your data folder e.g., for a revision after a year.",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#benefits",
    "href": "index.html#benefits",
    "title": "Welcome",
    "section": "Benefits",
    "text": "Benefits\nWe do know that the trend towards transparent and reproducible research is increasing, and more and more journals require researchers to publish the data on which their research results are based. The effort of publishing these data is negligible if you have a good data management strategy in place right from the beginning. Publishing FAIR data, i.e., data that are of actual use to others, also comes with numerous scientific benefits:\n\nData can be reused and cited in a timely manner. Citation of the data by other researchers increases visibility and can strengthen the reputation of your research.\nNew scientific collaborations may arise through data published according to FAIR, i.e., in reusable format. Collecting data is a time consuming business, so other researchers might as well benefit from already existing data (collected on public money), and you receive credit for producing the data.\nPublishing data on which research results are based (in addition to the ‘traditional’ scientific articles) will support the credibility, reproducibility, and validity of the results. This increases public trust in science.\n\nAdditional benefits are displayed in this graphic.\n\n\n\nSource: Hole, B. (2015). Open Science: A New publisher Perspective. Ubiquity press. (CC BY 4.0 International)",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to",
    "href": "index.html#how-to",
    "title": "Welcome",
    "section": "How to?",
    "text": "How to?\nSetting out this workshop, we quickly agreed that data management is best understood through learning by doing. For this reason, the whole workshop evolves around four published datasets:\n\nExample 1: https://edmond.mpg.de/dataset.xhtml?persistentId=doi:10.17617/3.1STIJV\nExample 2: https://data.ub.uni-muenchen.de/288/\nExample 3: https://osf.io/6p9bf/\nExample 4: https://zenodo.org/records/10650333\n\nIt is your job to investigate one of these datasets with a special focus on data organization, documentation, and publication aspects. We will conclude with an introduction to data management plans, a helpful planning tool to comply with the FAIR principles. In this section, we will also introduce important aspects of data storage.\nThe topics are accompanied by distinct boxes that are color-coded for their content:\n\n\n\n\n\n\nOrange boxes contain information crucial for that topic\n\n\n\n\n\n\n\n\n\n\n\n\nBlue boxes contain excursions to related topics\n\n\n\n\n\n\n\n\n\n\n\n\nGreen boxes contain hands-on exercises\n\n\n\n\n\n\n\n\n\n\n\n\nRed boxes contain the solutions and are collapsed\n\n\n\n\n\nOnly open the box if you want to see the solution!\n\n\n\nWe recommend that you look at the hands-on exercises first and see whether you already know their solutions. If things are unclear, you may return to the text anytime, but be aware that it is not necessary (and time will probably not permit) to read every paragraph and every box very carefully. The materials will, however, remain available, so you can always go back and reread more carefully!",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbased on: https://snd.se/en/manage-data/prepare-and-share/FAIR-data-principles↩︎\nbased on: https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/faire-daten/↩︎",
    "crumbs": [
      "About",
      "Welcome"
    ]
  },
  {
    "objectID": "Example3_READMETemplate.html",
    "href": "Example3_READMETemplate.html",
    "title": "GENERAL INFORMATION",
    "section": "",
    "text": "This readme file was generated on November 3, 2021 by Sarah Polk and Maike Kleemeyer\n\nGENERAL INFORMATION\nTitle of Data set: ICED MPMs Description: Investigating reliability of estimates of multiparameter mapping\nAuthor/Principal Investigator Information Name: Maike Kleemeyer ORCID: 0000-0002-9388-5535 Institution: n/a Email: n/a Date of data collection: n/a\nGeographic location of data collection: n/a\nInformation about funding sources that supported the collection of the data: n/a\n\n\nSHARING/ACCESS Data\nLicenses/restrictions placed on the data: No License\nLinks to publications that cite or use the data: https://doi.org/10.1101/2021.11.10.467254\nLinks to other publicly accessible locations of the data: https://git.mpib-berlin.mpg.de/plasticity/aktiv/hmri_longitudinal/tree/main\nLinks/relationships to ancillary data sets: n/a\nWas data derived from another source? If yes, list source(s): n/a\n\n\nDATA & FILE OVERVIEW\nFile List: - Gitlab The linked gitlab repository contains all scripts for processing the imaging data including descriptions of their purpose as well as the required folder structure to rerun the scripts. - Data The data folder contains the extracted data from the 11 different regions of interest (gray matter, white matter, pars triangularis of the inferior frontal gyrus, pars opercularis of the inferior frontal gyrus, orbitofrontal cortex, anterior cingulate cortex, precuneus, poster middle temporal gyrus, caudate, putamen, and pallidum) for each parameter map (MTsat, PD, R1, R2*) for each day (one and two) and each session (1 and 2). Regions of interest were taken from the Harvard-Oxford atlas as included in FSL 5.0. - extract_from_ROIs This folder contains the script used to extract means and standard deviations from the predefined regions of interest. - Scripts The scripts folder contains the models and ought to be run on the derived data in the data folder given the following structure: /data /data/MPMsExtractedWide.csv /data/MPMsExtractedWideStd.csv /scripts /scripts/CalcICCsWithCIs.R /scripts/ICED_MPMs_analysis.R\nRelationship between files, if important: n/a\nAdditional related data collected that was not included in the current data package: n/a\n\n\nMETHODOLOGICAL INFORMATION \nDescription of methods used for collection/generation of data:\nMethods for processing the data:\nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:"
  },
  {
    "objectID": "example-datasets.html",
    "href": "example-datasets.html",
    "title": "Example Datasets",
    "section": "",
    "text": "Example Datasets\nYour job is to investigate an existing dataset (discuss and note what is good or bad about it, which alternatives exist), to improve it, and to ‘publish’ the improved version. Note: As you shall not really publish the improved dataset in the end, the discussion is more important than the result. Do not hesitate to contact us for questions and for involving us in the discussion.\n\nChoose your dataset and download the data:\n\nDataset 1 (Groups 1, 5): https://doi.org/10.17617/3.1STIJV\n\nDataset 2 (Groups 2, 6): https://doi.org/10.5282/ubm/data.288\n\nDataset 3 (Groups 3, 7): https://osf.io/6p9bf/\n\nDataset 4 (Groups 4, 8): https://doi.org/10.5281/zenodo.10650332"
  },
  {
    "objectID": "Example2_READMETemplate.html",
    "href": "Example2_READMETemplate.html",
    "title": "GENERAL INFORMATION",
    "section": "",
    "text": "This readme file was generated on [YYYY-MM-DD] by Alexander Tekles\n\nGENERAL INFORMATION\nTitle of Data set: Data and code for “Same-gender citations do not indicate a substantial gender homophily bias” Description: This repository contains the (pre-processed) data and code that was used for generating the results presented in the paper “Same-gender citations do not indicate a substantial gender homophily bias”.\nAuthor/Principal Investigator Information Name: Tekles, Alexander ORCID: n/a Institution: n/a Email: alexander.tekles@soziologie.uni-muenchen.de &lt;Note: might be out of date since he is no longer listed among the staff members of the LMU Sociology department&gt;\nDate of data collection: n/a\nGeographic location of data collection: n/a\nInformation about funding sources that supported the collection of the data: n/a\n\n\nSHARING/ACCESS Data\nLicenses/restrictions placed on the data: CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)\nLinks to publications that cite or use the data: https://doi.org/10.1371/journal.pone.0274810\nLinks to other publicly accessible locations of the data: n/a\nLinks/relationships to ancillary data sets: n/a\nWas data derived from another source? If yes, list source(s):\n\nFaculty Opinions Data (Link?)\nWeb of Science (Link?)\n\n\n\nDATA & FILE OVERVIEW\nFile List:\n\nfocals.csv (data on the level of focal papers from the Faculty Opinions data)\nfocals_sections.csv (keywords provided in the Faculty Opinions data for each focal paper)\nfocal_pairs_fosims.csv (pairs of focal papers from the Faculty Opinions data)\nfocal_pairs_fosims_sc.csv (pairs of focal papers from the Faculty Opinions data; including self-citations)\nfocal_pairs_citedrefsims.csv (pairs of focal papers from the Faculty Opinions data)\nfocal_pairs_keywordsims.csv (pairs of focal papers from the Faculty Opinions data)\n\nfocal_pairs_subjsims.csv (pairs of focal papers from the Faculty Opinions data)\nfocal_pairs_fosims_cited.csv (pairs of focal papers from the Faculty Opinions data)\nfocal_pairs_fosims_cgender.csv (pairs of focal papers from the Faculty Opinions data; based on more restrictive gender assignments)\nfocal_pairs_abstrsims.csv (pairs of focal papers from the Faculty Opinions data\nwos_focal_pairs_abstrsims.csv (pairs of focal papers from WoS)\nanalyses_focals.R (This script produces the figure showing the estimates of the regression analyses on the level of focal papers. The script uses the files focals.csv and focals_sections.csv.)\nanalyses_pairs.R (This script produces figures showing the results of all analyses of paper pairs. In the first few lines, you can specify which results the script should produce. There are several vectors containing parameters representing the possible settings for which results can be produced. The script then iterates over all possible combinations of parameters. For example, in the vector fo_data_opts, the value T specifies to produce the results based on the Faculty Opinions data and the value F specifies to compute the results based on the WoS data. Changing the corresponding line from fo_data_opts &lt;- c(T, F) to fo_data_opts &lt;- c(T) would mean to only compute the results based on the Faculty Opinions data (instead of both the Faculty Opinions and the WoS data). This particular adjustment may be helpful because it saves a lot of runtime (the file containing the WoS data is much larger than the other files).)\nstudies_results.R (This script produces the figure showing the overview of results of previous studies on gender homophily in citations.)\n\nRelationship between files, if important:\nThe file gender_homophily_data_fo.tar.gz is a tarball that contains all data (CSV) files for the Faculty Opinions data. The file gender_homophily_data_wos.tar.gz contains all data (CSV) files for the Web of Science (WoS) data. The tarballs have to be unpacked to access the CSV files. This can be done via command line with the following commands:\ntar -xzf &lt;path_to_folder&gt;gender_homophily_data_fo.tar.gz -C &lt;path_to_target_folder&gt;\ntar -xzf &lt;path_to_folder&gt;gender_homophily_data_wos.tar.gz -C &lt;path_to_target_folder&gt;\nMake sure to replace &lt;path_to_folder&gt; with the path to the folder containing the tarballs and &lt;path_to_target_folder&gt; with the path to the folder where you want the CSV files to be unpacked.\nAll R scripts contain a function call of setwd() in the first few lines. The argument of this function should be the path to the folder containing the CSV files. This folder should also contain a folder named res, where the results of the analyses are saved.\nAdditional related data collected that was not included in the current data package: n/a\n\n\nMETHODOLOGICAL INFORMATION \nDescription of methods used for collection/generation of data:\nMethods for processing the data:\nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:"
  },
  {
    "objectID": "Example1_READMETemplate.html",
    "href": "Example1_READMETemplate.html",
    "title": "GENERAL INFORMATION",
    "section": "",
    "text": "This readme file was generated on [YYYY-MM-DD] by Mathias Göckede\n\nGENERAL INFORMATION\nTitle of Data set: Atmospheric mixing ratios of CO2 and CH4 at Ambarchik, NE Siberia, 2014-2017 Description: Ambarchik is located at the mouth of the Kolyma River, which opens to the East Siberian Sea (69.62N, 162.30E). The majority of the landscape in the immediate vicinity of the locality is wet tussock tundra. At the ecoregion scale, Ambarchik is bordered by Northeast Siberian Coastal Tundra ecoregion in the west, the Chukchi Peninsula Tundra ecoregion in the east, and the Northeast Siberian Taiga ecoregion in the south. Major components contributing to the net carbon exchange processes in the area are tundra landscapes including wetlands and lakes, as well as the Kolyma River and the East Siberian Arctic Shelf. Ambarchik hosts a weather station operated by the Russian meteorological service, whose staff is the entire permanent population of the locality. The closest town is Chersky (∼ 100 km to the south, with a population of 2857 as of 2010), with no other larger permanent settlement closer than 240 km. Thus, the site does not have any major sources of anthropogenic greenhouse gas emissions in the nearby. The only regular anthropogenic CO2 and potential CH4 sources that may influence the measurements are from the facility itseöf, including the building that hosts the power generator and the inhabited building. The atmospheric carbon observation station Ambarchik began operation in August 2014. It consists of a 27 m tall tower with two air inlets and meteorological measurements, while the majority of the instrumentation is hosted in a rack inside a building. Atmospheric mole fractions of CH4 , CO2 , and H2O are measured by an analyzer based on the cavity ring-down spectroscopy (CRDS) technique (G2301, Picarro Inc.), which is calibrated against WMO-traceable reference gases at regular intervals. The tower is located 260m from the shoreline, with a base elevation of 20 m a.s.l. For more detailed information on instrument setup and station characteristics, please refer to : Reum, F., Göckede, M., Lavric, J. V., Kolle, O., Zimov, S., Zimov, N., Pallandt, M., and Heimann, M.: Accurate measurements of atmospheric carbon dioxide and methane mole fractions at the Siberian coastal site Ambarchik, Atmos. Meas. Tech., 12, 5717-5740, 2019.\nAuthor/Principal Investigator Information Name: Göckede, Mathias ORCID: https://orcid.org/0000-0003-2833-8401 Institution: MPI for Biogeochemistry Email: mgoeck@bgc-jena.mpg.de\nDate of data collection: 2014-08 - 2017-12\nGeographic location of data collection: 69.62, 162.30\nInformation about funding sources that supported the collection of the data: n/a\n\n\nSHARING/ACCESS Data\nLicenses/restrictions placed on the data: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/)\nLinks to publications that cite or use the data: https://doi.org/10.5194/amt-12-5717-2019\nLinks to other publicly accessible locations of the data: n/a\nLinks/relationships to ancillary data sets: n/a\nWas data derived from another source? If yes, list source(s): n/a\n\n\nDATA & FILE OVERVIEW\nFile List:\nAMB hourly, readme.rtf (text/rtf) 1600 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_08_core-params.csv (text/comma-separated-values) 371048 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_02_core-params.csv (text/comma-separated-values) 343344 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_10_core-params.csv (text/comma-separated-values) 376873 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_05_core-params.csv (text/comma-separated-values) 377576 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_07_core-params.csv (text/comma-separated-values) 374412 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_10_core-params.csv (text/comma-separated-values) 377925 bytes. amb_hourly_qc_wc4.4_cal6.0_2014_10_core-params.csv (text/comma-separated-values) 294022 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_01_core-params.csv (text/comma-separated-values) 381443 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_12_core-params.csv (text/comma-separated-values) 380446 bytes. amb_hourly_qc_wc4.4_cal6.0_2014_09_core-params.csv (text/comma-separated-values) 361478 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_08_core-params.csv (text/comma-separated-values) 374008 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_04_core-params.csv (text/comma-separated-values) 371330 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_09_core-params.csv (text/comma-separated-values) 363410 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_12_core-params.csv (text/comma-separated-values) 347655 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_12_core-params.csv (text/comma-separated-values) 379224 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_04_core-params.csv (text/comma-separated-values) 365571 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_03_core-params.csv (text/comma-separated-values) 378853 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_02_core-params.csv (text/comma-separated-values) 342046 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_03_core-params.csv (text/comma-separated-values) 380693 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_11_core-params.csv (text/comma-separated-values) 357687 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_06_core-params.csv (text/comma-separated-values) 363493 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_04_core-params.csv (text/comma-separated-values) 361785 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_10_core-params.csv (text/comma-separated-values) 377013 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_03_core-params.csv (text/comma-separated-values) 382123 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_09_core-params.csv (text/comma-separated-values) 361469 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_07_core-params.csv (text/comma-separated-values) 371117 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_08_core-params.csv (text/comma-separated-values) 350376 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_06_core-params.csv (text/comma-separated-values) 364283 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_06_core-params.csv (text/comma-separated-values) 317891 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_05_core-params.csv (text/comma-separated-values) 378097 bytes. amb_hourly_qc_wc4.4_cal6.0_2014_11_core-params.csv (text/comma-separated-values) 365059 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_09_core-params.csv (text/comma-separated-values) 364376 bytes. amb_hourly_qc_wc4.4_cal6.0_2014_12_core-params.csv (text/comma-separated-values) 381365 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_05_core-params.csv (text/comma-separated-values) 371849 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_07_core-params.csv (text/comma-separated-values) 337815 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_11_core-params.csv (text/comma-separated-values) 366571 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_02_core-params.csv (text/comma-separated-values) 357049 bytes. amb_hourly_qc_wc4.4_cal6.0_2015_11_core-params.csv (text/comma-separated-values) 367144 bytes. amb_hourly_qc_wc4.4_cal6.0_2016_01_core-params.csv (text/comma-separated-values) 337952 bytes. amb_hourly_qc_wc4.4_cal6.0_2017_01_core-params.csv (text/comma-separated-values) 380469 bytes. amb_hourly_qc_wc4.4_cal6.0_2014_08_core-params.csv (text/comma-separated-values) 276139 bytes.\nRelationship between files, if important: n/a\nAdditional related data collected that was not included in the current data package: n/a\n\n\nMETHODOLOGICAL INFORMATION \nDescription of methods used for collection/generation of data:\nMethods for processing the data:\nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "2) Documentation",
    "section": "",
    "text": "Effective documentation is a crucial aspect of FAIR data management, ensuring that research data are not only well-organized but also easily Findable, Accessible, and Reusable by others. In this section, we’ll delve into the importance of documentation by exploring how to create best practice documentation that supports the entire research lifecycle. We’ll cover the essential components like including metadata that provide context and description of the dataset, as well as README files that offer a concise introduction to the dataset. Additionally, we’ll discuss the role of codebooks for making all components of the dataset self-explanatory.",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#overview",
    "href": "documentation.html#overview",
    "title": "2) Documentation",
    "section": "",
    "text": "Effective documentation is a crucial aspect of FAIR data management, ensuring that research data are not only well-organized but also easily Findable, Accessible, and Reusable by others. In this section, we’ll delve into the importance of documentation by exploring how to create best practice documentation that supports the entire research lifecycle. We’ll cover the essential components like including metadata that provide context and description of the dataset, as well as README files that offer a concise introduction to the dataset. Additionally, we’ll discuss the role of codebooks for making all components of the dataset self-explanatory.",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#metadata",
    "href": "documentation.html#metadata",
    "title": "2) Documentation",
    "section": "Metadata",
    "text": "Metadata\nMetadata are “data that provide information about other data”1, describing the context, content, and structure of a dataset. They contain essential information about the dataset, such as authorship, creation context, contents, provenance and accessibility. Metadata enable to understand and work with the data effectively. By providing a clear understanding of the data’s origin, and meaning, metadata play a critical role in ensuring data quality, reproducibility, and FAIRness.\n\n\n\n\n\n\nTask 2.1: (~10 min)\n\n\n\nCan you answer all of these questions by looking through your (example) dataset?\n(Concentrate on what you can find on the dataset level, don’t look into any README file yet.)\n\nWho created the dataset?\n\nWhat do the data files contain?\n\nWhen was the dataset generated?\n\nWhere was the dataset generated?\n\nWhy was the dataset generated?\n\nHow was the dataset generated?\n\nWhat research is this dataset connected to?\n\nCan this dataset be re-used? \n\nDiscuss:\nWhere did you find this information?\nWas everything readily understandable?\nBonus:\nTime yourself - how long does it take you to answer all questions?\n\n\n\n\n\n\n\n\nTips on where to find metadata\n\n\n\nMetadata can be found at several places:\n\nRepository description: You are often asked about entering metadata, when you upload your dataset to a repository. The quality and extent of metadata you can provide varies from repository to repository. We will cover repositories in Chapter 3).\nREADME file: Already before you publish your dataset, you should keep track of your metadata, in order to prevent loosing the overview yourself, and for informing others. A good way to do that is creating a README file. We will cover README files in the next section.\nRelated publication: Most datasets that are published are connected to scientific papers, which detail what has been done to generate the dataset, what it consists of and its analysis process. While this can provide very detailed information, it makes it difficult to assess the dataset on its own.\n\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\nAnswers from different sources have been brought together.\n\nWho created the dataset?\n\nGöckede, Mathias; Reum, Friedemann; Heimann, Martin\n\nWhat do the data files contain?\n\nAtmospheric mixing ratios of CO2 and CH4 at Ambarchik, NE Siberia,\nTabular data about measurements taken at the meteorological station for each month.\n\n\nWhen was the dataset generated?\n\nThe data were collected between August 2014 and December 2017.\nThe dataset was published in 2023\n\nWhere was the dataset generated?\n\nAt the meteorological station in Ambarchik. Ambarchik is located at the mouth of the Kolyma River, which opens to the East Siberian Sea (69.62N, 162.30E).\n\nWhy was the dataset generated?\n\nUnclear from the dataset alone\nFrom the related publication: Sparse data coverage in the Arctic hampers our understanding of its carbon cycle dynamics and our predictions of the fate of its vast carbon reservoirs in a changing climate. In this paper, we present accurate measurements of atmospheric carbon dioxide (CO2) and methane (CH4) dry air mole fractions at the new atmospheric carbon observation station Ambarchik, which closes a large gap in the atmospheric trace gas monitoring network in northeastern Siberia.\n\nHow was the dataset generated?\n\nThe atmospheric carbon observation station Ambarchik consists of a 27 m tall tower with two air inlets and meteorological measurements, while the majority of the instrumentation is hosted in a rack inside a building. Atmospheric mole fractions of CH4 , CO2 , and H2O are measured by an analyzer based on the cavity ring-down spectroscopy (CRDS) technique (G2301, Picarro Inc.), which is calibrated against WMO-traceable reference gases at regular intervals.\n\nWhat research is this dataset connected to?\n\nReum, F., Göckede, M., Lavric, J. V., Kolle, O., Zimov, S., Zimov, N., Pallandt, M., and Heimann, M.: Accurate measurements of atmospheric carbon dioxide and methane mole fractions at the Siberian coastal site Ambarchik, Atmos. Meas. Tech., 12, 5717-5740, 2019.\nhttps://doi.org/10.5194/amt-12-5717-2019 (Not provided on dataset level)\n\nCan this dataset be re-used?\n\nYes, under the conditions of the CC BY 4.0 License\n\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\nAnswers from different sources have been brought together.\n\nWho created the dataset?\n\nTekles, Alexander; Auspurg, Katrin; Bornmann, Lutz\n\nWhat do the data files contain?\n\nThis repository contains the data and code that was used for generating the results presented in the paper “Same-gender citations do not indicate a substantial gender homophily bias”.\nFurther information is unclear.\n\nWhen was the dataset generated?\n\nThe dataset was uploaded on February 16th 2022\nNo information about the time frame of the data creation on the dataset level.\n\nWhere was the dataset generated?\n\nNot indicated on dataset level\n\nWhy was the dataset generated?\n\nNot indicated on dataset level\nFrom the related publication: When controlling for research topics at a high level of granularity, there is only little evidence for a gender homophily bias in citation decisions. Our study points out the importance of controlling structural aspects such as gendered specialization in research topics when investigating gender bias in science.\n\nHow was the dataset generated?\n\nNot indicated on dataset level\nFrom the related publication: The bibliometric data used in this paper are from an in-house database developed and maintained in cooperation with the Max Planck Digital Library (MPDL, Munich) and derived from the Science Citation Index Expanded (SCI-E), Social Sciences Citation Index (SSCI), Arts and Humanities Citation Index (AHCI) prepared by Clarivate Analytics (Philadelphia, Pennsylvania, USA).\n\nWhat research is this dataset connected to?\n\nArticle: “Same-gender citations do not indicate a substantial gender homophily bias”\nhttps://doi.org/10.1371/journal.pone.0274810 (Not provided on dataset level)\n\nCan this dataset be re-used?\n\nThe dataset is under a CC BY-NC-SA 4.0 License, which makes its reuse highly restricted.\n\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\nAnswers from different sources have been brought together.\n\nWho created the dataset?\n\n\nSarah Polk; Maike Kleemeyer (0000-0002-9388-5535)\n\nWhat do the data files contain?\n\n\nThis repository contains data and code for the manuscript entitled “Reliability of quantitative multiparameter maps is high for MT and PD but attenuated for R1 and R2* in healthy young adults”\nThe data folder contains the extracted data from the 11 different regions of interest (gray matter, white matter, pars triangularis of the inferior frontal gyrus, pars opercularis of the inferior frontal gyrus, orbitofrontal cortex, anterior cingulate cortex, precuneus, poster middle temporal gyrus, caudate, putamen, and pallidum) for each parameter map (MTsat, PD, R1, R2*) for each day (one and two) and each session (1 and 2). Regions of interest were taken from the Harvard-Oxford atlas as included in FSL 5.0.\n\nWhen was the dataset generated?\n\n\nThe data repository was created on November 3, 2021 and last modified on March 12, 2022\nFurther information on the time frame of the data creation is unclear on dataset level.\n\nWhere was the dataset generated?\n\n\nNot indicated on dataset level.\n\nWhy was the dataset generated?\n\n\nNot indicated on dataset level.\nFrom the related publication: In sum, in this sample of healthy young adults, the four MPM parameters showed very little variability over four measurements over two days but differed in how well they could assess between-person differences. We conclude that R1 and R2* might carry only limited person-specific information in samples of healthy young adults, and, by implication, might be of restricted utility for studying associations to between-person differences in behavior.\n\nHow was the dataset generated?\n\n\nNot indicated on dataset level.\nFrom the related publication: We acquired data from 15 volunteers, who each were assessed four times. On Day 1, participants were measured twice back-to-back, without repositioning between MPM datasets. On Day 2, participants were also measured twice, but this time with a break in between measurements, which afforded repositioning of the participants’ head.\n\nWhat research is this dataset connected to?\n\n\nArticle: “Reliability of quantitative multiparameter maps is high for MT and PD but attenuated for R1 and R2* in healthy young adults”\nhttps://doi.org/10.1101/2021.11.10.467254 (Not provided on dataset level)\n\nCan this dataset be re-used?\n\n\nThe provided dataset does not have a license. Re-use is therefore unclear.\n\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\nAnswers from different sources have been brought together.\n\nWho created the dataset?\n\n\nDominik Deffner; Benjamin (Kahl)\n\nWhat do the data files contain?\n\n\nNot indicated on dataset level.\nFrom file structure: Data from 40 Group sessions and 40 solo sessions with relating scripts.\nFrom related Github repository: This repository contains the full data and scripts to reproduce all analyses and figures as well as the experimental game itself used in Deffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. & Kurvers, R. (accepted, in principle, at Nature Communications): “Collective incentives reduce over-exploitation of social information in unconstrained human groups”\n\nWhen was the dataset generated?\n\n\nFrom file structure: Sessions were between July 5th, 2022 and September 21, 2022.\nThe dataset was published on February 12, 2024.\n\nWhere was the dataset generated?\n\n\nNot indicated on dataset level.\nFrom the related publication: At the Max Planck Institute for Human Development\n\nWhy was the dataset generated?\n\n\nNot indicated on dataset level.\nFrom the related publication: Collective dynamics emerge from countless individual decisions. Yet, we poorly understand the processes governing dynamically-interacting individuals in human collectives under realistic conditions. We present a naturalistic immersive-reality experiment where groups of participants searched for rewards in different environments, studying how individuals weigh personal and social information and how this shapes individual and collective outcomes.\n\nHow was the dataset generated?\n\n\nNot indicated on dataset level.\nFrom related publication: 160 participants were recruited from the Max Planck Institute for Human Development (MPIB) recruitment pool and invited in anonymous groups of four to the behavioral laboratory at the MPIB in Berlin, Germany (63 identified as male, 97 as female; Mage = 28.5, SDage = 6.4 years; all were proficient in German and most came from Western, educated, industrialized, rich, and democratic societies70,71). Fourty additional participants were recruited for an individual control condition (14 identified as male, 26 as female; Mage = 29.8, SDage = 5.7 years). See further section: “Procedure”\n\nWhat research is this dataset connected to?\n\n\nNot indicated on dataset level.\nFrom related GitHub Repository: Deffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. & Kurvers, R. (accepted, in principle, at Nature Communications): “Collective incentives reduce over-exploitation of social information in unconstrained human groups”\nhttps://doi.org/10.1038/s41467-024-47010-3 (Not provided on dataset level)\n\nCan this dataset be re-used?\n\n\nThe dataset is under a CC BY 4.0, which would allow re-use under the given conditions.\nHowever, the GitHub repository is connected to an Apache License 2.0, which makes it difficult to know which license applies.\n\n\n\n\n\n\n\n\n\n\n\n\nExcursion: Community Standards\n\n\n\nCommunity standards for metadata allow for improved comparison between datasets. Some disciplines have agreed-upon formal standards that define how metadata should be documented. If you publish your data via a discipline specific repository, it is likely that their entry form already guides you through the community metadata standard. You can look for metadata standards that are specific to your discipline e.g. via FAIRsharing.org. If a standard for your research field exists, you should follow it as good as possible.\nEven if your discipline does not adhere to strict standards, there is an advantage in using generic standards. By using controlled vocabularies or ontologies for your metadata you can increase the machine-readability and -actionability of your data.\nMachine-readability refers to the ability of machines (computers, algorithms, etc.) to automatically understand and interpret the meaning of metadata, such as keywords, descriptions, and concepts, without human intervention. Machine-actionability takes it a step further, enabling machines to not only understand the metadata but also to perform actions or take decisions based on that metadata, such as data integration, filtering, or visualization, without human intervention.",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#readme-file",
    "href": "documentation.html#readme-file",
    "title": "2) Documentation",
    "section": "README file",
    "text": "README file\nWe already established that metadata and README files are strongly connected. They form a great way to provide an introduction to and documentation of a dataset and its research context. A README file is a plain text file often written in markdown and then named “README.md”. This naming convention helps spotting the README file for humans and machines alike, e.g. in GitHub repositories the README.md file is identified automatically and its content displayed on the repositories main page. The README file should be one of the first files your create for a new project. You can then continue filling it with the relevant information.\nWhat should be in a README file? - Ideally all the answers to the questions posed under the metadata section should be given in the README file. To determine what should be best included in your README file, it can also be helpful to go through the questions of a (Research) Data Management Plan. While your README file should be very descriptive, it is also important to stay concise and clear. Avoid e.g. using jargon to improve inter-disciplinary usage.\n\n\n\n\n\n\nTask 2.2: (~10 min)\n\n\n\n\nDownload and open the README file of your example dataset in a text editor of your choice.\nCopy the template into another text editor file.\nWhere does the provided information of the example README differ from the template below? Count the sections of the template that you can answer solely from the provided README. (The sections will be marked as **bold** in the example solution files.)\nHow many of the 19 open fields of the template could you answer based on your solutions from task 2.1?\nDiscuss: What information can be found in the README example that is not contained in the template but contributes to understanding the dataset?\n\nFor brevity reasons we exclude the last section “METHODOLOGICAL INFORMATION” from the provided example solutions.\n\n\n\n\n\n\n\n\nREADME Template\n\n\n\n\n\nThis readme file was generated on [YYYY-MM-DD] by [NAME]\n&lt;help text in angle brackets should be deleted before finalizing your document&gt; &lt;[text in square brackets should be changed for your specific dataset]&gt;\n# GENERAL INFORMATION\nTitle of Dataset:\nDescription: &lt;provide a short description of the study/project&gt;\n&lt;provide at least one contact&gt;\nAuthor/Principal Investigator Information\nName:\nORCID:\nInstitution:\nEmail:\n\nDate of data collection: &lt;provide single date, range, or approximate date; suggested format YYYY-MM-DD&gt;\nGeographic location of data collection: &lt;provide latitude, longitude, or city/region, State, Country&gt;\nInformation about funding sources that supported the collection of the data:\n# SHARING/ACCESS Data\nLicenses/restrictions placed on the data:\nLinks to publications that cite or use the data:\nLinks to other publicly accessible locations of the data:\nLinks/relationships to ancillary datasets:\nWas data derived from another source? If yes, list source(s):\n# DATA & FILE OVERVIEW\nFile List: &lt;list all files (or folders, as appropriate for dataset organization) contained in the dataset, with a brief description&gt;\nRelationship between files, if important:\nAdditional related data collected that was not included in the current data package:\n# METHODOLOGICAL INFORMATION\nDescription of methods used for collection/generation of data: \nMethods for processing the data: \nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:\n\n\n\n\n\n\n\n\n\nTips for your own README\n\n\n\nFor your own research project you can, of course, adapt this template to include the information that is relevant to your own dataset. Also look out for other projects in your discipline and what they include in their README files.\nYou might also suggest to your department/research group/institute/research community to set up a discipline specific README template that can improve the interoperability, reusability and comparability of your research projects.\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\n1 section can be filled based on the Example dataset README alone.\nExcluding the “METHODOLOGICAL INFORMATION” section, ~12 of the 19 open fields can be filled based on the information collected in task 2.1.\nFind the filled out README template for example 1 here\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\n2 sections can be filled based on the Example dataset README alone.\nExcluding the “METHODOLOGICAL INFORMATION” section, ~10 of the 19 open fields can be filled based on the information collected in task 2.1.\nFind the filled out README template for example 2 here\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\n5 sections can be filled based on the Example dataset README alone.\n\nExcluding the “METHODOLOGICAL INFORMATION” section, ~10 of the 19 open fields can be filled based on the information collected in task 2.1.\nFind the filled out README template for example 3 here\nNote: Only the README file for the OSF repository was taken into consideration.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\n4 sections can be filled based on the Example dataset README alone.\nExcluding the “METHODOLOGICAL INFORMATION” section, ~11 of the 19 open fields can be filled based on the information collected in task 2.1.\nFind the filled out README template for example 4 here",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#codebook",
    "href": "documentation.html#codebook",
    "title": "2) Documentation",
    "section": "Codebook",
    "text": "Codebook\nWhereas general metadata and README information typically focus on the dataset or project as a whole, codebooks delve deeper, providing detailed explanations and clarifications about the specific data contained in individual files. A codebook should make all components of a dataset file self-explanatory. It helps to identify e.g. what certain variables and value labels mean or in which format the data points have been collected. This enhances the FAIRness of the dataset, because it allows to re-use it independent of the availability of an associated research paper.\nSome people include this information in the README file. However, this can significantly extent the length of the README. We recommend to keep README files concise to maintain their readability.\nIf all of your data files are structured and labeled according to the same system it is enough to provide one codebook that describes and explains this system. If you have differently structured files you should consider providing multiple codebooks.\nHere are some of the elements that can be included in a codebook:\n\nvariables\nvalue labels\nmeasuring unit\ndata types\ndata format\ndata dictionary\ndata quality notes\ntechnical info\n\n\nBasic level:\nA first step is to provide this information about your data is in a .csv table, a .txt file or .md (markdown) file. These are easy to create via a text editor of your choice. They can be opened by everyone independent of proprietary systems and are easily readable by humans.\n\n\n\n\n\n\nExample of a simple .csv codebook\n\n\n\n\n\n“variable”,“data_type”,“format”\n“species”,“string”,“Adelie,Chinstrap,Gentoo”\n“population_size”,“integer”,“0-100000”\n“location”,“string”,“Latitude, Longitude”\n“date”,“string”,“YYYY-MM-DD”\n“latitude”,“number”,“-90 to 90”\n“longitude”,“number”,“-180 to 180”\n\n\n\n\n\n\nAdvanced level:\nDepending on the complexity of your data, you can improve your codebook by providing it as a JSON file. This has the advantage to be machine-readable and -actionable. It makes it easier to extract this information and work with it in coding scripts.\n\n\n\n\n\n\nExample of a detailed JSON code book\n\n\n\n\n\n{\n  \"title\": \"Penguin Populations Code book\",\n  \"description\": \"Code book for the Penguin Populations dataset\",\n  \"variables\": [\n    {\n      \"name\": \"species\",\n      \"description\": \"The species of penguin\",\n      \"data_type\": \"string\",\n      \"format\": [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n    },\n    {\n      \"name\": \"population_size\",\n      \"description\": \"The number of penguins in the population\",\n      \"data_type\": \"integer\",\n      \"format\": \"0-100000\"\n    },\n    {\n      \"name\": \"location\",\n      \"description\": \"The region or location where the population is found\",\n      \"data_type\": \"string\",\n      \"format\": \"Latitude, Longitude\"\n    },\n    {\n      \"name\": \"date\",\n      \"description\": \"The date when the population size was recorded\",\n      \"data_type\": \"string\",\n      \"format\": \"YYYY-MM-DD\"\n    },\n    {\n      \"name\": \"latitude\",\n      \"description\": \"The latitude of the location\",\n      \"data_type\": \"number\",\n      \"format\": \"-90 to 90\"\n    },\n    {\n      \"name\": \"longitude\",\n      \"description\": \"The longitude of the location\",\n      \"data_type\": \"number\",\n      \"format\": \"-180 to 180\"\n    }\n  ],\n  \"data_dictionary\": {\n    \"species\": {\n      \"Adelie\": \"A small to medium-sized penguin species found in Antarctica\",\n      \"Chinstrap\": \"A medium-sized penguin species found in the Antarctic and sub-Antarctic\",\n      \"Gentoo\": \"A large penguin species found in the Antarctic and sub-Antarctic\"\n    }\n  },\n  \"data_quality_notes\": [\n    \"Population sizes are estimates and may not reflect the actual number of penguins in the population.\",\n    \"Locations are approximate and may not reflect the exact location of the population.\",\n    \"Dates are in the format YYYY-MM-DD and are in the UTC timezone.\"\n  ],\n  \"technical_info\": {\n    \"format\": \"JSON\",\n    \"compression\": \"gzip\",\n    \"required_software\": \"JSON parser\"\n  }\n}\n\n\n\n\n\n\n\n\n\nTask 2.3: (~10 min)\n\n\n\n\nOpen one of the data files of your example dataset.\nCan you make sense of the data provided? Note down difficulties you identify.\nCan you find supportive information to understand the provided data in other documents? Make a note of them.\n\n\n\n\n\n\n\n\n\nSolution: Example 1\n\n\n\n\n\n\nThe column names within the data files are not very descriptive. However, all data files work with the same columns.\nExplanations of the column names, variables, as well as data types an formats are given in the example’s README file.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 2\n\n\n\n\n\n\nIt is a very large dataset and files are compressed. Explanation on how to extract the data is provided.\nExplanations for the column names of the different files are provided in the example’s README file.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 3\n\n\n\n\n\n\nThe data files can be open directly in the OSF repository. The column names are not very descriptive.\nHowever each data file has a corresponding codebook in .json format that explains each of the columns.\nThe scripts, how to deploy them and how to generate result from the data is explained in the README file.\n\n\n\n\n\n\n\n\n\n\nSolution: Example 4\n\n\n\n\n\n\nThe data files are all .log files that can be opened with a standard editor as ;-separated tables.\nColumns are named relatively clear.\nThe scripts, how to deploy them and how to generate result from the data is explained in the README-file.",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#further-resources",
    "href": "documentation.html#further-resources",
    "title": "2) Documentation",
    "section": "Further Resources:",
    "text": "Further Resources:\n\n“Documentation and Metadata”, The Turing Way, accessed 7. July 2024. https://book.the-turing-way.org/reproducible-research/rdm/rdm-metadata",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "2) Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n„metadata“. Merriam-Webster Dictionary, accessed 7. July 2024, https://www.merriam-webster.com/dictionary/metadata.↩︎",
    "crumbs": [
      "About",
      "2) Documentation"
    ]
  },
  {
    "objectID": "dmp.html",
    "href": "dmp.html",
    "title": "4) Data Management Plans",
    "section": "",
    "text": "As the previous sections have shown, data management is a broad topic encompassing various aspects. Hence, good planning is essential to effectively manage data according to the FAIR principles. Writing a (research) data management plan (DMP) is ideal to predefine how data will be handled. This part of the workshop provides an overview of the components of a DMP, offers tips on creating one, and introduces helpful tools. It concludes with a practical exercise where you will work on a small section of a DMP.",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#overview",
    "href": "dmp.html#overview",
    "title": "4) Data Management Plans",
    "section": "",
    "text": "As the previous sections have shown, data management is a broad topic encompassing various aspects. Hence, good planning is essential to effectively manage data according to the FAIR principles. Writing a (research) data management plan (DMP) is ideal to predefine how data will be handled. This part of the workshop provides an overview of the components of a DMP, offers tips on creating one, and introduces helpful tools. It concludes with a practical exercise where you will work on a small section of a DMP.",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#general-information",
    "href": "dmp.html#general-information",
    "title": "4) Data Management Plans",
    "section": "General information",
    "text": "General information\nA data management plan is a written document describing the data you expect to collect or use during a research project. Further, a DMP shows how you will manage, describe, analyze, and store the data, as well as the mechanisms for sharing and preserving data when your project is complete. Thus, a data management plan accompanies and guides you throughout the project and encompasses all phases that research data go through in a project. Following the motto “start early, refine often”, creating a DMP at the beginning of your project enables it to be a living document that evolves alongside your research. Begin with basic information and enrich the document as your project progresses. Note that it is not required to answer all questions; stick to those relevant to your data. DMPs apply to any research project, from a PhD thesis to large collaborative research projects.",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#components-of-a-dmp",
    "href": "dmp.html#components-of-a-dmp",
    "title": "4) Data Management Plans",
    "section": "Components of a DMP",
    "text": "Components of a DMP\nThroughout a project, there are different processing stages regarding data. You can illustrate the process as a life cycle with the following stages:\n\n\n\n\n\n\n\nResearch data life cycle by Jürgen Rohrwild\n\n\nA data management plan covers all stages of the research data life cycle by assigning questions to each stage. Addressing the questions relevant to your research project will result in a comprehensive DMP. Exemplary questions include:\n\nWhat data will be collected, and where do they come from?\nWhich metadata should be used to describe the data?\nWhere should the data be stored? During the project phase? After the project?\nWhat about Open Data? Do you plan to publish the data?\nDoes your project require personnel resources for data management? Does your project require special infrastructure?\nHow do you ensure the quality of your data?\nIs your research project concerned with personal or sensitive data? What laws apply? Are there ethical aspects you need to consider?1\n\nPlease note that this list is incomplete, and the questions may vary depending on your discipline and data specifics.\n\n\n\n\n\n\nExcursion: Example DMPs\n\n\n\nLooking at published DMPs, especially from your discipline, may be helpful to gain a deeper insight into the topic. Have a look at the following list to find some example DMPs. (Note: some of these collections are not uptodate.)\n\nArgos Published DMPs\nDigital Curation Centre Example DMPs and Guidance\nPublic DMPs by DMPonline\nLIBER Europe DMP Catalogue\nExamples for Horizon 2020 DMPs by the University of Vienna\nData management plans in the RIOjournal",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#example-data-storage-and-security",
    "href": "dmp.html#example-data-storage-and-security",
    "title": "4) Data Management Plans",
    "section": "Example: Data storage and security",
    "text": "Example: Data storage and security\nIn this section, we approach a DMP from a practical perspective: In the following, we will look at one specific section of a DMP, namely “Data storage and security” and provide example answers for two datasets. Given that this is quite a relevant topic for data management that we have not yet covered in the previous sections, we start by providing some background information on data storage and security. (If you already feel comfortable with the topic, you can skip this part.)\nRegarding data storage and security, the following three “S” encompass the topic: safe - secure - sustainable. Now, we dive into the main principles of each part:\n\nSafe\nSafety focuses on the protection of data against physical loss.2 To ensure your data’s safety and recoverability, establish a regular backup routine. This involves creating copies of your data at specified times and storing them separately from the originals, ideally adhering to the 3-2-1 rule (see below). Consider using external hard drives or USB sticks for local backups.3 Additionally, if your institution offers backup systems like LRZ cloud storage or LRZ Sync+Share, you can store your data where they have automated backup mechanisms. Another synched storage is Keeper, a service available for all Max Planck employees.\n\n\n\n\n\n\n3-2-1 rule\n\n\n\nThe 3-2-1 backup rule states that you should maintain three copies of your data on two different storage mediums (e.g. local storage and external hard drive), with one copy stored in a remote location. A remote copy can be stored in the cloud.\n\n\nRemember, your backups should encompass your data files and include software applications if necessary. This ensures a comprehensive recovery if needed.4\n\n\nSecure\nProtecting data against unauthorized access and data misuse is one of the main goals of security.5 We recommend using access control mechanisms and encryption to prevent unauthorized access to data and to protect sensitive information. Confidential data is sensitive information that could cause harm if disclosed, such as details about endangered species, vulnerable archaeological sites, or personal information. Personal data is subject to strict privacy laws (cp. General Data Protection Regulations) and requires secure handling. To protect sensitive data, it is crucial to minimize the risk of identification through techniques like pseudonymization (i.e., replace personally identifiable information by a pseudonym) or anonymization (i.e., complete removal of personally identifiable information from the research data). Additionally, strong password protection using reliable password management tools is essential to prevent unauthorized access.6\n\n\nSustainable\nData sustainability aims at making data persistent across technological and human evolution.7 Concretely, this means choosing file formats that are unlikely to become obsolete and ensuring the long-term readability and accessibility of your data. For more details on this topic, revisit Part 1: Organization / File formats.\n\n\nExample answers for a DMP\nThe following examples are intended to show how specific questions can be answered in a DMP. The questions are part of the RDMO template, section “Data usage / Data storage and security” and “Data usage / Interoperability”. For each question, we will provide you with example answers for two datasets. These answers do not rely on any existing project. Also, the data and some tools (e.g. software) are invented. The example answers are based on the information in this example DMP.\n\n\n\n\n\n\nExample data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: The simulations use the open-source “QuantumNeuroBrain” (made up) software suite. We determine potentials and signals for various environments. The simulations vary the environment parameters (ion concentrations, temperature, …) and consider eight different models for the transduction mechanism proposed in the literature (also not relying on actual literature research). The simulations themselves follow from input files (for models, parameters, etc.) and configuration files that capture the used settings in software and hardware. The simulation output is further analyzed and forms the basis for illustrative images. Input, configuration files, output and derived data, such as images, are intimately connected and are considered a single diverse dataset.\nDataset “EEG measurements”: This dataset includes high-resolution EEG measurements from up to 100 probands. Additional measurements will accompany the EEG measurements to determine the environmental conditions (see previous dataset). The measurements will be performed using a Neuro++ machine (made up), allowing for state-of-the-art sensitivity. Existing measurements will supplement the dataset from a local neuroscience group. We use the open-source tool EEGalyse (made up) to analyze and visualize the results.\n\n\n\n\nSafe:\n\nHow and how often will backups of the data be created? (This question refers to backups while the data is being worked with only, not long-term preservation.)\n\n\n\n\n\n\n\nAnswer for example data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: While all data are initially stored on local servers in the research institute, all data that pass the initial quality checks are additionally transferred to servers of the university computing centre. Here, automatic backup services are available daily.\nDataset “EEG measurements”: All data are stored on local servers in the research institute. The RAID system provides redundancy, and all data are backed up daily. Fully anonymized data are also stored on IT centre servers to add a layer of redundancy.\n\n\n\n\n\nWho is responsible for the backups? (This question refers to backups while the data is being worked with only, not long-term preservation.)\n\n\n\n\n\n\n\nAnswer for example data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: All project members themselves store their data on local servers in the research institute. If the data passes the initial quality checks, all researchers transfer their data to the university computing centre.\nDataset “EEG measurements”: All project members themselves store their data on local servers in the research institute. The backup is then done automatically. Furthermore, the research data manager transfers the fully anonymized data to the IT centre.\n\n\n\n\nSecure:\n\nWho is allowed to access the dataset (e.g., project members, partners of the project, only in-house, external partners)?\n\n\n\n\n\n\n\nAnswer for example data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: Only project members are allowed to access the data.\nDataset “EEG measurements”: Only persons who are part of the EEG group of the project team are allowed to access the sensitive data. All folders are, therefore, password-protected. A detailed list of the persons who can access the data are stored in the project filesystem. Once the data is anonymized, the research manager transfers it to the IT centre, where the IT centre staff and all project members can access the data.\n\n\n\n\n\nWhich measures or provisions are in place to ensure data security (e.g. protection against unauthorized access, data recovery, transfer of sensitive data)?\n\n\n\n\n\n\n\nAnswer for example data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: \n\nProtection against unauthorized access: Detailed access and rights management via the university identity management (IdM) system\nData recovery - backups: Automated backups to university computing centre servers\n\nDataset “EEG measurements”:\n\nProtection against unauthorized access: Detailed access and rights management via the university IdM system, additional password protection of sensitive data\nData robustness - encryption: Transfer of data between systems only in encrypted form (AES encrpytion)\nData recovery - backups: Each encrypted and fully anonymized dataset is backed up by IT centre servers. The local secure RAID system provides redundancy for sensitive data during all stages of data handling.\nSensitive data - anonymization or pseudonymization: Access to sensitive (not fully anonymous) data is only possible for the EEG group members and on a computer physically located in the rooms of the research group.\nOther: The research data manager will ensure that all group members are comfortable with the established data handling procedures, particularly regarding sensitive information, as well as using the backup service and documentation of all data handling and versioning steps.\n\n\n\n\n\nSustainable:\n\nIs this dataset interoperable, e.g. allowing data exchange and reuse between researchers, institutions, organizations, countries, etc.? (Note that this question covers a broader scope than just sustainability.)\n\nThe dataset adheres to standardized formats: …\nThe dataset is usable with available (open) software applications or software applications that are established and widely used in the respective community: …\nThe dataset can easily be re-combined with different datasets from different origins: …\nOther aspects in terms of interoperability: …\n\n\n\n\n\n\n\n\nAnswer for example data\n\n\n\n\n\n\nDataset “Simulations of neural signal transmission”: \n\nStandardized formats: Input and simulation output are stored in CSV format. Derived datasets, such as cleansed or aggregated data from multiple simulations and statistical analyses, are stored in standard CSV formats. All illustrative images created from the data are stored as jpg. Each data file is accompanied by a ReadMe (plain text) file that provides information on the structure of the associated data file, such as column descriptions, units, software version, initial configuration, etc.\nUsable with available open / established, widely used software applications: Since all files use standardized formats, no special software is needed.\nRe-combination with different datasets from different origins:  Notebooks and scripts are provided that can be used to retrace the more involved analysis steps and include other data. The scripts can be found on GitHub [Link to group Git]. The notebooks and the scripts require no specific tool beyond basic and free software (python, emacs, …).\nOther aspects in terms of interoperability:\n\nDataset “EEG measurements”:\n\nStandardized formats: The created EEG data records are converted to and initially stored as BrainVision Core Data Format triples (.vhdr, .vmrk, .eeg). Additional information, such as information extracted from the manufacturer-provided data files, is recorded as JSON files. The data will automatically be transformed to the EEG-BIDS standard, which allows for combination with the existing EEG dataset.\nUsable with available open / established, widely used software applications: Since all files use established and highly standardized formats, the data can be used in the scientific community. The needed software is broadly available, and open-source/free solutions exist.\nRe-combination with different datasets from different origins: Reusing the data in other research contexts requires the entire dataset (pre-anonymization), which can only be made available upon detailed ethical approval.\nOther aspects in terms of interoperability:",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#why-is-a-dmp-helpful",
    "href": "dmp.html#why-is-a-dmp-helpful",
    "title": "4) Data Management Plans",
    "section": "Why is a DMP helpful?",
    "text": "Why is a DMP helpful?\nCreating a DMP may seem like (and certainly is) a lot of work, but it comes with numerous benefits for you and your research:\n\nEnhanced organization: A DMP forces you to think through important aspects before your project actually starts, ensuring that your data is well-documented, organized, and easily accessible throughout the project.\nOne living document: It contains all information regarding data management for your project, so you can always go back, look things up, make changes or additions but you can also pass it on to e.g. new project members instead of explaining the same things over and over.\nImproved reproducibility: Thorough documentation promotes transparency and facilitates replication of your research findings.\nEfficient data sharing: A DMP helps considering important aspects for data publication (e.g., having participants’ consent) and therefore simplifies data sharing once the project ends.\n\nAs mentioned, a DMP addresses crucial technical, organizational, legal, and financial considerations for your project right from the beginning and provides a comprehensive overview of all potential data management tasks. It structures your workflow and builds the basis for optimizations. Also, a DMP supports your own good scientific work by ensuring your data is complete, accurate and trustworthy.\nFurthermore, a data management plan can be part of applications and reports to third-party-funded projects. When applying for grants, submitting a DMP may be mandatory (see Excursion: Funder requirements). Beyond that, more and more institutions adopt guidelines for handling research data. You can find an overview of universities in Bavaria with a research data policy here.\nIn conclusion, a DMP is a helpful tool that streamlines your workflow and supports you in fostering research integrity.\n\n\n\n\n\n\nExcursion: Funder requirements\n\n\n\nMany funding agencies ask for information on handling research data. The following table8 provides an overview of the requirements of German and European funding agencies:\n\nOverview of funder requirements\n\n\nFunder\nDMP demanded?\nSubmission on application?\nContent\nUpdates?\n\n\n\n\nEuropean Commission, Horizon Europe\nYes\nComprehensive plan within the first 6 month of the project\nHorizon Europe DMP template; Information and template for ERC grants\nUpdate, if significant changes occur and at the end of the project\n\n\nGerman Research Foundation (DFG)\nInformation on the handling of research data\nYes\nContents of the DFG Guidelines on the Handling of Research Data\nNo\n\n\nGerman Federal Ministry of Education and Research (BMBF)\nPlan sometimes required, depending on the programme\nIf required, yes\nContent depends on the respective programme; Educational research: Checklist (in German)\nDepends on the programme\n\n\nVolkswagen Stiftung (VW Foundation)\nYes\nYes\nTemplate for a Base-DMP\nNo",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#dmp-tool-rdmo",
    "href": "dmp.html#dmp-tool-rdmo",
    "title": "4) Data Management Plans",
    "section": "DMP tool: RDMO",
    "text": "DMP tool: RDMO\nDifferent software is available to streamline the DMP creation process, simplify workflows, and empower collaborative work.\n\n\n\n\n\n\nDMP tool vs. repository\n\n\n\nIt is essential to clarify the difference between a DMP tool and a repository: A DMP tool is a planning tool where you gather information about research data management in general, e.g. about data collection, description, etc. You can also use it as a documentation tool but it does not store your actual research data. A repository in contrast is a platform where you eventually store and publish your data.\n\n\nOne of these tools is the Research Data Management Organiser (RDMO). RDMO is an open-source online tool designed to create, store and print your DMP. It supports you in planning projects and administrating data management tasks throughout the whole data life cycle. The tool is an outcome of a project co-funded by the German Research Foundation (DFG) between 2015 and 2020 and is constantly developed and optimized. The University Library of LMU Munich and the Max Planck Digital Library offer instances of the web-based software on their servers (which means both institutions offer their “own” RDMO application). You can register using your email or ORCID for LMU RDMO or your MPG login (Shibboleth) for MPG RDMO.\nKey features of RDMO:\n\nTemplates and guidance: To create a DMP (called a project in RDMO), choose one of the pre-built templates that are designed as questionnaires. The questions highlight the aspects we discussed earlier and guide you step-by-step through the creation process. Some questionnaires are based on the requirements of different research funding agencies, e.g., the German Research Foundation (DFG).\nCollaboration-friendly: It is possible to work collaboratively on one DMP as a team. Therefore, RDMO supports different roles for the project members: visitor, author, manager and owner. The owner can edit and delete the project, whereas a visitor has read-only rights.\nImport and export: Furthermore, RDMO offers import and export features. A DMP can be exported to formats like PDF, XML, or Markdown. The exported document provides a good guide for internal documentation and could be a suitable tool for reporting to the funding agency.\n\nVersion control: RDMO stores only the current version of a DMP, but the “Snapshot” feature allows you to manually track changes and revert to previous versions, giving you full control over your data management plan.\n\nFurther information on the features of RDMO can be found here: RDMO Quick Start Guide by University Library of LMU Munich (Download, PDF) and Quick Start to RDMO for MPG.\n\n\n\n\n\n\nExcursion: Further DMP tools\n\n\n\nIn addition to RDMO, a variety of DMP tools exist. The following table9 provides an overview of discipline-agnostic DMP tools that are freely available online:\n\nOverview of DMP tools\n\n\nName\nProvider\nTemplates\n\n\n\n\n\nARGOS\nOpenAIRE, EUDAT\nCHIST-ERA\n\n\n\nDMPonline\nDigital Curation Center (DCC)\nEuropean programs (e.g. Horizon Europe), funding agencies from different European countries, mainly the UK; DDC template\n\n\n\nDMPTool\nUniversity of California, California Digital Library\nmostly US funding agencies; DCC template\n\n\n\nData Stewardship Wizard\nELIXIR CZ / ELIXIR NL\nHorizon Europe, Science Europe, maDMP\n\n\n\nGFBio DMPT\nGFBio\n-\n\n\n\n\n\n\n\n\n\n\n\n\nTask 4: (~20 min)\n\n\n\nIf you do not yet have a RDMO account, please register now. Either use the LMU RDMO (register using your email or ORCID id) or MPG RDMO (MPG login: Shibboleth). Login and create a new project using the RDMO catalog.\nAnswer the questions for a subset of the following sections for the dataset you previously worked on:\n\nContent classification / Datasets\nContent classification / Re-use\nTechnical classification / Formats\nData usage / Data organization\nData usage / Data sharing and re-use\nMetadata and referencing / Metadata\n\nYou can summarize the outcome of the previous tasks and discuss your thoughts with your group members.  \n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe do not provide a solution for this task as it summarizes the previous tasks. Also, note that there is no correct DMP. Try to be as precise as possible and provide detailed information on your project and research data. Depending on your data and discipline, some questions may be obsolete and do not need to be answered.",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "dmp.html#footnotes",
    "href": "dmp.html#footnotes",
    "title": "4) Data Management Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuestions based on: https://www.fdm.uni-hamburg.de/fdm/datenmanagementplan.html↩︎\nbased on: https://www.baramundi.com/de-de/blog/artikel/unterschied-safety-security/↩︎\nbased on: https://www.forschungsdaten.info/themen/speichern-und-rechnen/datensicherheit-und-backup/↩︎\nbased on: https://www.forschungsdaten.info/themen/speichern-und-rechnen/datensicherheit-und-backup/↩︎\nbased on: https://www.baramundi.com/de-de/blog/artikel/unterschied-safety-security/↩︎\nbased on: https://doi.org/10.17605/OSF.IO/GDQ93 and https://www.forschungsdaten.info/themen/speichern-und-rechnen/datensicherheit-und-backup/↩︎\nbased on: https://doi.org/10.1016/j.infoandorg.2023.100449↩︎\nbased on: https://doi.org/10.5281/zenodo.5773203↩︎\nbased on: https://forschungsdaten.info/themen/informieren-und-planen/datenmanagementplan/.↩︎",
    "crumbs": [
      "About",
      "4) Data Management Plans"
    ]
  },
  {
    "objectID": "Example4_READMETemplate.html",
    "href": "Example4_READMETemplate.html",
    "title": "GENERAL INFORMATION",
    "section": "",
    "text": "This readme file was generated on [YYYY-MM-DD] by Dominik Deffner\n\nGENERAL INFORMATION\nTitle of Data set: Collective incentives reduce over-exploitation of social information in unconstrained human groups Description: This repository contains the full data and scripts to reproduce all analyses and figures as well as the experimental game itself used in Deffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. & Kurvers, R. (accepted, in principle, at Nature Communications): “Collective incentives reduce over-exploitation of social information in unconstrained human groups”\nAuthor/Principal Investigator Information Name: Deffner, Dominik ORCID: Institution: Email: Date of data collection: 2022-07-05 to 2022-09-21\nGeographic location of data collection: Max Planck Institute for Human Development\nInformation about funding sources that supported the collection of the data: n/a\n\n\nSHARING/ACCESS Data\nLicenses/restrictions placed on the data: CC BY 4.0 / Apache License 2.0\nLinks to publications that cite or use the data: https://psyarxiv.com/p3bj7/\nLinks to other publicly accessible locations of the data: https://github.com/DominikDeffner/VirtualCollectiveForaging/tree/0.1.0?tab=readme-ov-file\nLinks/relationships to ancillary data sets: n/a\nWas data derived from another source? If yes, list source(s): n/a\n\n\nDATA & FILE OVERVIEW\nFile List:\n“master_file.r” sources all other scripts and implements the entire project workflow from raw Unity data to plots in the manuscript.\nThe “Scripts” folder contains all relevant R scripts for data processing and analysis:\n\n“functions.R” loads all required functions and packages\n“data_prep.R” runs data preparation script to construct dataframes for analysis from raw Unity outputs\n“HiddenMarkovModels_prep.R” contructs extended dataframe and stan data list for Social Hidden Markov Decision model\n“Behavior_prep.R” computes behavioral summary statistics for each player/group and round\n“Behavior_individual_level.R” runs and plots behavioral analyses at individual level\n“Behavior_group_level.R” runs and plots behavioral analyses at group level\n“Behavior_main_plot” produces main behavioral plot (Fig.2)\n“Behavior_solitary.R” runs and plots behavioral analyses for solo control condition\n“HiddenMarkovModels_baseline.R” runs baseline Social Hidden Markov Decision model and produces main text plots\n“Viterbi.r” computes most likely state sequences for all participants through Viterbi algorithm\n“HiddenMarkovModels_ESM_Plots.R” produces supplementary HMM plots\n“HiddenMarkovModels_temporal.R” runs and plots temporal Social Hidden Markov Decision model\n“CollectiveDynamics.R” runs and plots collective visual-spatial dynamics analyses using time-lagged Gaussian-process regressions\n\nThe “Stan model code” folder contains stan files for the (baseline and time-varying) Social Hidden Markov Decision model as well as the time-lagged Gaussian-process model - “m_SHMDM.stan” is the baseline HMM (called in “HiddenMarkovModels_baseline.R”) - “m_SHMDM_temporal.stan” is the time-varying HMM (called in “HiddenMarkovModels_temporal.R”) - “m_temporal_success.stan” is monotonic-effect model for success over time (called in “HiddenMarkovModels_temporal.R”) - “m_time_laggedGP.stan” is the time-lagged Gaussian-process model to analyze collective visual-spatial dynamics (called in “CollectiveDynamics.R”)\nThe “Data” folder contains all raw data for both “Group” and “Solo” conditions. Each sub-folder contains data from one experimental session. There are separate .txt files for participant demographics and for player and patch data from each round (indexing starts at 0). Variable names should be quite descriptive, but please get in touch in case anything is unclear.\nRelationship between files, if important: included above\nAdditional related data collected that was not included in the current data package: n/a\n\n\nMETHODOLOGICAL INFORMATION\nDescription of methods used for collection/generation of data:\nMethods for processing the data:\nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "3) Publication",
    "section": "",
    "text": "Data publication is the final step of FAIR data management, ensuring data Findability and Accessibility (when done well). If you have taken care of the previous sections Data organization and Documentation and thereby made your data Interoperable and Reusable, publishing those data will take minimal effort.\nMost commonly, data are published as supplements to journal articles and an increasing number of journals actually require that. Datasets can also be published in specialized data journals (e.g., Scientific data, Data in Brief), which means that the article itself is a rich and detailed description of the dataset. As you may have guessed, this option is mainly chosen for rather large, sampling-intensive datasets. Last but not least, it is also possible to publish an independent (without connected publication) dataset in a repository and this is often required by funders like the DFG or EU. Irrespective of the publication option, there are common good practices when publishing data, namely\n\nindicating in the article that data and/or code are available,\ndefining usage conditions,\nensuring reliable access,\nchoosing an appropriate repository.\n\nThese will be handled in the course of this chapter.\n\n\n\n\n\n\nLegal basis\n\n\n\nEven though publishing data (and code) is a prerequisite for transparent and reproducible research do note that in some cases the legal basis may prevent you from doing so. This most commonly concerns copyright or data protection issues. E.g., someone else may have the copyrights on the data you are working with and they do not allow the data to be published. Or when working with personal data (i.e., data from identifiable living people), you would violate subjects’ rights according to the general data protection regulation (GDPR) when publishing these.\nThere is a very nice decision tree (unfortunately only in German) that can help you find out whether and under what conditions you can publish your dataset. Broadly speaking, publishing personal data is only possible if either they can be fully anonymized (and no longer fall under GDPR) or if participants consented to their data being published. It is hence important to consider even before collecting personal data, whether they ought to be published to eventually include that in the consent form. If this topic is of interest to your research also check out Felix Schönbrodt’s lecture on “Data anonymity” from the Open Research Summer School 2023.\nIn seldom cases publishing data can also be limited due to concerns of national security. You should take a few minutes to consider whether your research could be misused by foreign powers. Also consider getting in contact with your supervisor, research department and/or legal department to clarify whether elements of export control policies might apply to your research.",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#overview",
    "href": "publication.html#overview",
    "title": "3) Publication",
    "section": "",
    "text": "Data publication is the final step of FAIR data management, ensuring data Findability and Accessibility (when done well). If you have taken care of the previous sections Data organization and Documentation and thereby made your data Interoperable and Reusable, publishing those data will take minimal effort.\nMost commonly, data are published as supplements to journal articles and an increasing number of journals actually require that. Datasets can also be published in specialized data journals (e.g., Scientific data, Data in Brief), which means that the article itself is a rich and detailed description of the dataset. As you may have guessed, this option is mainly chosen for rather large, sampling-intensive datasets. Last but not least, it is also possible to publish an independent (without connected publication) dataset in a repository and this is often required by funders like the DFG or EU. Irrespective of the publication option, there are common good practices when publishing data, namely\n\nindicating in the article that data and/or code are available,\ndefining usage conditions,\nensuring reliable access,\nchoosing an appropriate repository.\n\nThese will be handled in the course of this chapter.\n\n\n\n\n\n\nLegal basis\n\n\n\nEven though publishing data (and code) is a prerequisite for transparent and reproducible research do note that in some cases the legal basis may prevent you from doing so. This most commonly concerns copyright or data protection issues. E.g., someone else may have the copyrights on the data you are working with and they do not allow the data to be published. Or when working with personal data (i.e., data from identifiable living people), you would violate subjects’ rights according to the general data protection regulation (GDPR) when publishing these.\nThere is a very nice decision tree (unfortunately only in German) that can help you find out whether and under what conditions you can publish your dataset. Broadly speaking, publishing personal data is only possible if either they can be fully anonymized (and no longer fall under GDPR) or if participants consented to their data being published. It is hence important to consider even before collecting personal data, whether they ought to be published to eventually include that in the consent form. If this topic is of interest to your research also check out Felix Schönbrodt’s lecture on “Data anonymity” from the Open Research Summer School 2023.\nIn seldom cases publishing data can also be limited due to concerns of national security. You should take a few minutes to consider whether your research could be misused by foreign powers. Also consider getting in contact with your supervisor, research department and/or legal department to clarify whether elements of export control policies might apply to your research.",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#data-availability-statement",
    "href": "publication.html#data-availability-statement",
    "title": "3) Publication",
    "section": "Data Availability Statement",
    "text": "Data Availability Statement\nA Data Availability Statement (as well as a Code Availability Statement) is a special section in an article that states whether the authors have made the evidence supporting their findings available, and if so, where readers may access it. They are usually placed somewhere towards the end of the article, mostly before the Reference section. As part of their commitment to supporting open research, some journals now require all manuscripts to include a Data Availability Statement in order to be accepted for publication. Even if a journal does not require such a statement, it is highly recommendable to include it and make transparent that the data underlying the article’s findings are available1!\n\n\n\n\n\n\nDouble-blind peer review process\n\n\n\nWhen submitting to a journal that uses a double-blind peer review process, it’s important to ensure that the information in the Data Availability Statement doesn’t compromise the anonymity of you or your co-authors. If there is information in your Data Availability Statement that could be used to identify the manuscript authors (e.g., by linking to a repository that reveals author information), make sure to ask the journal what they would like you to do2.\n\n\n\n\n\n\n\n\nTask 3.1:\n\n\n\n\nIs your dataset the basis for a published article?\nHow did you find out?\nIf so, does the article include a Data Availability Statement?\nWhere did you find it?\n\n\n\n\n\n\n\n\n\nSolution: Group 1 (~ 5 minutes)\n\n\n\n\n\n\nYes, this dataset is the basis for a published article.\nIn the meta data field “Related Publication”.\nYes, it does but without linking it to the data on Edmond. You should rather contact one coauthor without providing contact details.\nBefore the Appendices.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 2 (~ 5 minutes)\n\n\n\n\n\n\nYes, this dataset is the basis for a published article.\nAs part of the title.\nYes.\nIn the very beginning following Abstract and Figures. However, no special section therefore not very intuitive to find.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 3 (~ 5 minutes)\n\n\n\n\n\n\nYes, this dataset is the basis for a published article.\nIt is stated in the README.\nYes.\nIn the “Open Research” section before the Reference section.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 4 (~ 5 minutes)\n\n\n\n\n\n\nYes, this dataset is the basis for a published article.\nIn “Related works”, a link to a github repo is provided and when going there it is stated in the README.\nYes.\nAs separate section before “Code availability” and the Reference section.",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#license",
    "href": "publication.html#license",
    "title": "3) Publication",
    "section": "License",
    "text": "License\nIf we in the end would like to share our research data and enable reuse for other researchers, we should define conditions under which the data can be reused. At this point licenses come into play. Licenses are standard contracts that regulate usage rights for published work. As already mentioned, thereby enabling other people to reuse the published work. If no license is provided with a dataset, it is not clear to others under which conditions they can reuse it and they might (against your good intentions) rather refrain from reusing it at all.\n\n\n\n\n\n\nLicense and Copyright\n\n\n\nIt is important to keep in mind that you can only define usage conditions by assigning licenses for work that “is yours”, i.e., that you have the copyright for. This is not always the case for data. Data that result from a significant creative process (as is often the case in the humanities or qualitative social sciences) are more likely covered by copyright while data that result from rather simple measurement processes (as is often the case for natural or quantitative social sciences) are likely not covered. Also, note the distinction between the instrument used to create the data and the data themselves in this regard. E.g., if the data you are about to publish result from a comprehensive questionnaire that you have developed, the questionnaire ought to be protected by copyright whereas the data are most likely not. As mentioned in the beginning, you can only assign licenses to work that you hold the copyright for. If this is not the case or you are unsure, it is safest to go with CC0 as this is strictly speaking not a license, but rather a waiver of copyright. You explicitly release your work into the public domain.\n\n\nThere are several licenses out there, as e.g. Creative Commons, MIT (software), Apache (software), Open Data Commons and it ultimately is your responsibility to pick one. If you have no advise and you do not want to read through all licenses out there, the Creative Commons licences are usually a save choice (see box below). As rule of thumb, you may want to remember to go with the least restrictive license to maximize your data’s reuse potential. A recent article gives the following recommendations3:\n\nProvide open research data under the CC-BY license or the CC0 label.\nIf it can be assumed that research data are not protected by copyright, CC0 should be preferred.\n\nEven if CC BY is possible copyright-wise, be aware that it merely paraphrases the rules of good scientific practice. So you are setting conditions that are rather self-evident for scientists but nevertheless imply an extra effort for reuse, namely figuring out how to attribute correctly according to the license.\nFor similar reasons, using more restrictive licenses should be extremely carefully considered, at least if you are intending to make your data reusable for others. In particular, it can become impossible to merge data published under different, more restrictive licenses as their licenses may be incompatible, preventing follow-up research, archiving or even publication of the combined data (cf. this CC FAQ).\nCC0 is great, because it is universally compatible and ensures free reusability of research data.\n\n\n\n\n\n\nExcursion: Creative Commons licences\n\n\n\nCreative commons licenses are widely used in research and we will therefore go into a little bit of detail on these. CC provides four features, which form the basis of a fixed set of six CC licenses. These are:\n\nBY: For “attribution”, i.e., the obligation to credit the author and other parties designated for attribution\nND: For “no derivatives”, i.e., only verbatim copies of the work can be shared\nNC: For “non-commercial”, i.e., commercial use is excluded from the license grant\nSA: For “share alike”, i.e., the work can be modified and modified versions can be published but only under the original or a compatible license\n\nThe following six licenses result from the combination of these four features, listed from most (left) to least (right) permissive here:\n\n\n\n\n\n\n\n\n\nCC BY\n\n\n\n\n\n\n\nCC BY-SA\n\n\n\n\n\n\n\nCC BY-NC\n\n\n\n\n\n\n\nCC BY-NC-SA\n\n\n\n\n\n\n\nCC BY-ND\n\n\n\n\n\n\n\nCC BY-NC-ND\n\n\n\n\n\nIn addition, there are two further labels, to dedicate the published work to the public domain.\n\nCC0: Allowing others to reuse your work without conditions, i.e., even if you were copyright holder you are not insisting on it. Good scientific practice obviously foresees that you will in any case credit the author(s) of work you are (re)using!\nPublic domain mark: Explicitly label work that is not restricted by copyright.\n\n\n\n\n\n\n\n\n\n\nCC0\n\n\n\n\n \n\n\n\n\n\nPDM\n\n\n\n\n \n\n\n\nAs stated above, the CC0 label or the CC BY license should be your preferred choice. In (the unlikely) case that you need a more restrictive license, there is a nice graphical illustration to help you: choo-choo-choose your license.\n\n\n\n\n\n\n\n\nTask 3.2:\n\n\n\n\nAre you actually allowed to reuse your dataset?\nIf so, under which conditions?\n\n\n\n\n\n\n\n\n\nSolution: Group 1 (~ 2 minutes)\n\n\n\n\n\n\nYes.\nCC BY 4.0.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 2 (~ 2 minutes, if delving deeper into the conditions obviously longer)\n\n\n\n\n\n\nYes.\nCC BY-NC-SA 4.0.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 3 (~ 5 minutes)\n\n\n\n\n\n\nNo.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 4 (~ 2 minutes)\n\n\n\n\n\n\nYes.\nCC BY 4.0. However the git repo is under a Apache License Version 2.0, so depending on which version you use, the conditions are different.",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#persistent-identifier",
    "href": "publication.html#persistent-identifier",
    "title": "3) Publication",
    "section": "Persistent identifier",
    "text": "Persistent identifier\nImagine a scenario in which a repository is no longer maintained, so that all datasets published in that repository are no longer available at the repository’s URLs (a quite real scenario, as you can see in this article). To avoid data loss, the original repository migrated its datasets to another repository, where it however gets a new URL. Consequently, if the corresponding article linked to its dataset via the URL (e.g., https://osf.io/gn47c/), it can no longer be found. What now? In such cases, persistent identifiers (PIDs) are extremely helpful if not essential. The ones we consider crucial are listed below.\n\nDigital object identifier\nA digital object identifier (DOI) points directly to a digital object rather than to its location online (unlike URL). You may know it already from publications. It consists of a unique number made up of a prefix and a suffix separated by a forward slash as e.g., 10.5281/zenodo.4322849. It allows things to be uniquely identified and accessed reliably by using the DOI proxy server https://doi.org/ and appending the DOI, e.g., https://doi.org/10.5281/zenodo.4322849.\n\n\nOpen Researcher and Contributor ID\nThe Open Researcher and Contributor ID (ORCID) provides a unique and persistent identifier that you yourself connect with your professional (scientific) information. This helps to recognize all your work, even if e.g., you change your name, one of your coauthors misspells it, or you move on to a new institution. Getting an ORCID is free and quick and meanwhile, several platforms, from grant application to manuscript submission and beyond, allow you to use your ORCID to log into their system. Note however, that this is a self-maintained identifier. It thus is only as up-to-date as you keep it.\n\n\nResearch Organization Registry\nLast but not least, the Research Organization Registry (ROR) is a global, community-led registry of open persistent identifiers for research organizations. The LMU has one and all Max Planck Institutes have one as well. These are much easier to use than entering all the affiliation information whenever asked for it and at the same time help disambiguate institutional affiliations.\n\n\n\n\n\n\nTask 3.3:\n\n\n\n\nCan your dataset be accessed reliably?\nWho would you contact, if questions concerning the dataset arose?\nIs that contact still available?\nDoes your institution have a ROR?\n\n\n\n\n\n\n\n\n\nSolution: Group 1 (~ 5 minutes)\n\n\n\n\n\n\nYes, via DOI https://doi.org/10.17617/3.1STIJV.\nThe first author as listed corresponding author in the article.\nUnsure, he has moved on but he also connected his ORCID profile, which is however not well maintained.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 2 (~ 5 minutes)\n\n\n\n\n\n\nYes, via DOI https://doi.org/10.5282/ubm/data.288.\nThere is a contact person listed in the metadata with corresponding email address.\nUnsure, he is not listed at the Institut für Soziologie anymore, but he also connected his ORCID profile in the article.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 3 (~ 5 minutes)\n\n\n\n\n\n\nNo, it only has a URL.\nThe first author as listed corresponding author in the article.\nUnsure, she has moved on, but there is no alternative.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 4 (~ 5 minutes)\n\n\n\n\n\n\nYes, via DOI https://doi.org/10.5281/zenodo.10650333.\nThe first author as listed corresponding author in the article.\nYes, and he also connected his ORCID profile.",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#repository",
    "href": "publication.html#repository",
    "title": "3) Publication",
    "section": "Repository",
    "text": "Repository\nRepositories are database systems to document, store and publish digital objects. Thereby, they ensure visibility (via metadata) and sustainability (via use of PIDs) of the digital objects they are holding. Repositories can be:\n\nDiscipline-specific (e.g. OpenNeuro)\nGeneral (e.g. Zenodo, OSF)\nInstitutional (e.g. Edmond MPG, Open Data LMU)\n\nIf available, discipline-specific repositories are preferable, because these are commonly used in a specific research field and researchers will most likely start searching for data in those repositories. The “Registry of research data repositories” (Re3data.org) provides a collection of repositories with very nice search and overview functions. E.g., you can browse by subject (i.e., discipline).\n\n\n\nre3data.org\n\n\nWhen looking for a repository there are various criteria that should be considered.\n\nCriteria\n\nIs the data completely open? Can I control access or set an embargo period?\nUnder what license can I publish data?\nDoes the repository provide a persistent identifier?\nIs the repository certified as an indicator for its sustainability?\nWhat are the costs?\nWhat are the user rights?\nWhere are the data hosted? Is this compliant with the General Data Protection Regulation?\nIs there a limit to the amount of data I can upload?\n\nWhen searching re3data, most of these criteria are displayed beside the repository via icons, so you can immediately check whether a potential repository fulfills your requirements.\n\n\n\nCriteria icons in re3data.org\n\n\n\n\n\n\n\n\nTask 3.4:\n\n\n\n\nWhich repository was used to publish your dataset?\nCan you find that repository on re3data.org?\nUnder what criteria can you publish your data in that repository?\nIs there a discipline-specific repository for your research field?\n\n\n\n\n\n\n\n\n\nSolution: Group 1 (~ 5 minutes)\n\n\n\n\n\n\nEdmond.\nYes.\nClick on the repository and go through the details: It assigns a DOI, you can publish under various licenses, it is certified etc.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 4 (~ 5 minutes)\n\n\n\n\n\n\nOpen Data LMU.\nYes.\nClick on the repository and go through the details: It assigns a DOI, you can publish under various licenses, it is certified etc.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 3 (~ 5 minutes)\n\n\n\n\n\n\nOSF.\nYes.\nClick on the repository and go through the details: It assigns a DOI, you can publish under various licenses, it is certified etc.\n\n\n\n\n\n\n\n\n\n\nSolution: Group 4 (~ 5 minutes)\n\n\n\n\n\n\nZenodo.\nYes.\nClick on the repository and go through the details: It assigns a DOI, you can publish under various licenses, it is certified etc.\n\n\n\n\n\n\n\n\n\n\nExcursion: FAIR assessment tools\n\n\n\nWith the emerging requisite of research data being FAIR, a number of tools have been developed that automatically check how FAIR published data are. These tools have been evaluated and a nice overview has been published (c.p. https://zenodo.org/records/7701941). We invite you to play around with these tools, especially the automatic tools F-uji Tool and FAIR enough. E.g., you may input your dataset to different tools and look at the output, check whether the output confirms your impression of the dataset’s reusability, or find a dataset that has been published in different repositories and see how that influences a tool’s output (e.g., https://doi.org/10.5281/zenodo.7298798 vs. https://doi.org/10.17617/3.LLWRWR.)",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "publication.html#footnotes",
    "href": "publication.html#footnotes",
    "title": "3) Publication",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource https://www.cambridge.org/core/services/authors/open-data/data-availability-statements.↩︎\nSource https://www.cambridge.org/core/services/authors/open-data/data-availability-statements.↩︎\nBrettschneider, P., Axtmann, A., Böker, E., & von Suchodoletz, D. (2021): Offene Lizenzen für Forschungsdaten: Rechtliche Bewertung und Praxistauglichkeit verbreiteter Lizenzmodelle, in: O-Bib. Das Offene Bibliotheksjournal 8(3), pp. 1–22, https://doi.org/10.5282/o-bib/5749. [only in German]↩︎",
    "crumbs": [
      "About",
      "3) Publication"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This material was created for the LMU & MPG Open Science Summer School 2024. During the Summer School the material was accompanied by slides and the tasks were done in groups. Independent of this event, the materials can be used to get an insight into Research Data Management in the context of Open Science and therefore with the goal of a FAIR publication.\nThe materials can be used, remixed, adapted and shared according to the CCBY4.0 Licence."
  }
]